{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Program to download the docx files from links inside an excel file"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook repertoriate all the intent to download the files, the first cell is the good one, all the rest is commented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import time\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# File path\n",
    "download_file = r\"../data/raw/full_data_link_legifrance.xlsx\"\n",
    "\n",
    "# Input directory\n",
    "list_url = pd.read_excel(download_file)[\"UrlLegifrance\"].tolist()\n",
    "\n",
    "# Code\n",
    "prefs = {\n",
    "    \"download.default_directory\": r\"../data/raw/text/docx\", \n",
    "}\n",
    "\n",
    "\n",
    "for url in tqdm(list_url, desc=\"Downloading\", unit=\"file\"):\n",
    "    try:\n",
    "        options = webdriver.ChromeOptions()\n",
    "        options.add_experimental_option(\"prefs\", prefs)\n",
    "        options.add_argument(\"--headless\")\n",
    "        driver = webdriver.Chrome(options=options)\n",
    "        driver.get(url)\n",
    "        html = driver.page_source\n",
    "        soup = bs(html, 'html.parser')\n",
    "        download_url = 'https://www.legifrance.gouv.fr' + soup.find_all(class_=\"doc-download\")[0].attrs['href']\n",
    "        driver.get(download_url)\n",
    "        time.sleep(1)\n",
    "        driver.quit()\n",
    "    except Exception as e:\n",
    "        print(\"Error occurred:\", str(e))\n",
    "        continue\n",
    "\n",
    "print(\"====================================\")\n",
    "print(\"Download finished\")\n",
    "print(\"====================================\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V1 this code is functionnal and take the form of a function. The main problem is the presence of an Incapsula firewall. I am currently trying to circumvent it."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This program is done to automitically download all files from an excel sheet and put them into a new folder.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # import libraries to deal with an excel file, open the link and download the file\n",
    "# import os\n",
    "# import pandas as pd\n",
    "# import requests\n",
    "# from bs4 import BeautifulSoup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function is used to download from an url links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def download_docx(url, path):\n",
    "#     response = requests.get(url) # open the link and get the HTML content\n",
    "#     soup = BeautifulSoup(response.content, \"html.parser\") # parse the HTML content into a readable format\n",
    "#     download_link = soup.findAll(\"a\", attrs={\"class\": \"doc-download\", \"target\": \"_blank\"}) # find the download link in the HTML content\n",
    "#     if download_link and \"href\" in download_link.attrs:  # check if the download link exists\n",
    "#         download_url = download_link[\"href\"] # get the download link inside the href attribute\n",
    "#         response = requests.get(download_url) # open the download link\n",
    "#         if response.ok:\n",
    "#             with open(f\"{path}/document.txt\", \"wb\") as f:\n",
    "#                 f.write(response.content)\n",
    "#             print(\"Document downloaded successfully.\")\n",
    "#         else:\n",
    "#             print(\"Failed to download the document. Status code:\", response.status_code)  \n",
    "#     else:\n",
    "#         print(\"Download link not found.\")            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download_docx(\"https://www.legifrance.gouv.fr/acco/id/ACCOTEXT000046679509\", \"/Users/GARSON/Dares-Alpha/\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we would like to build a function that will select the first column of the excell sheet and download the word document from the link"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V2 It is an algorithm not taking a function form and having the same issue with Incapsula"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a function I used for inspiration and found useful. Source: https://www.geeksforgeeks.org/implementing-web-scraping-python-beautiful-soup/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this is code is able to collect data since the webpage (URL) is not protected by a firewall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "# import csv\n",
    "\n",
    "# URL = \"http://www.values.com/inspirational-quotes\"\n",
    "# r = requests.get(URL)\n",
    "\n",
    "# soup = BeautifulSoup(r.content, 'html.parser')\n",
    "# print(soup.prettify())\n",
    "\n",
    "# quotes=[] # a list to store quotes\n",
    "\n",
    "# table = soup.find('div', attrs = {'id':'all_quotes'}) # get the table having the quotes\n",
    "\n",
    "# for row in table.findAll('div', attrs = {'class':'col-6 col-lg-4 text-center margin-30px-bottom sm-margin-30px-top'}):\n",
    "#     quote = {}  # create an empty dictionary to store quote information\n",
    "#     quote['theme'] = row.h5.text  # extract the 'theme' from the h5 element within the row\n",
    "#     quote['url'] = row.a['href']  # extract the 'url' from the 'href' attribute of the 'a' element within the row\n",
    "#     quote['img'] = row.img['src']  # extract the 'img' from the 'src' attribute of the 'img' element within the row\n",
    "#     quote['lines'] = row.img['alt'].split(\" #\")[0]  # extract the 'lines' from the 'alt' attribute of the 'img' element within the row, splitting by \" #\" to remove the author part\n",
    "#     quote['author'] = row.img['alt'].split(\" #\")[1]  # extract the 'author' from the 'alt' attribute of the 'img' element within the row, splitting by \" #\" to remove the lines part\n",
    "#     quotes.append(quote)  # add the quote dictionary to the quotes list\n",
    "\n",
    "# filename = 'inspirational_quotes.csv'\n",
    "# with open(filename, 'w', newline='') as f:\n",
    "#     w = csv.DictWriter(f,['theme','url','img','lines','author'])  # create a DictWriter object to write the quotes to a CSV file\n",
    "#     w.writeheader()  # write the header row to the CSV file\n",
    "#     for quote in quotes:\n",
    "#         w.writerow(quote)  # write each quote as a row in the CSV file"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I propose an example of what is happining when I try to get the HTML structure of the webpage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests # to get the html code of the page\n",
    "# import csv # to write/read the data in csv format\n",
    "# from bs4 import BeautifulSoup # to parse html code\n",
    "# import os   # to create folders\n",
    "\n",
    "# #URL = \"https://www.legifrance.gouv.fr/acco/id/ACCOTEXT000047508298\"\n",
    "# URL = \"https://www.legifrance.gouv.fr/acco/id/ACCOTEXT000038514173?dateSignature=&init=true&page=1&query=salaires&searchField=ALL&tab_selection=acco\"\n",
    "# r = requests.get(URL)\n",
    "# soup = BeautifulSoup(r.content, 'html.parser')\n",
    "# print(soup.prettify())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that we obtain a non tractable HTML source code. My interpretation is that I am facing a firewall."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here comes the first attempt by using zenrows. But the result is not plainly satisfactory (code is not super clean) and this solution has a limited amount of trial while being free (1000 free credits and it costs 25 per use of the API key)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pip install requests\n",
    "# import requests\n",
    "\n",
    "# url = \"https://www.legifrance.gouv.fr/acco/id/ACCOTEXT000038514173?dateSignature=&init=true&page=1&query=salaires&searchField=ALL&tab_selection=acco\"\n",
    "# proxy = \"http://d6eae4e7da98c6e73e959de1f47e0e0f4b4c0b0e:antibot=true&premium_proxy=true@proxy.zenrows.com:8001\"\n",
    "# proxies = {\"http\": proxy, \"https\": proxy}\n",
    "# response = requests.get(url, proxies=proxies, verify=False)\n",
    "# print(response.text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I try a second attempt by setting a headers to fake a visit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "# from fake_useragent import UserAgent\n",
    "\n",
    "# from fake_useragent import UserAgent\n",
    "# import requests\n",
    "   \n",
    "\n",
    "# ua = UserAgent()\n",
    "# header = {'User-Agent':str(ua.chrome)}\n",
    "# url = \"https://www.legifrance.gouv.fr/acco/id/ACCOTEXT000038514173?dateSignature=&init=true&page=1&query=salaires&searchField=ALL&tab_selection=acco\"\n",
    "# r = requests.get(url, headers=header)\n",
    "# print(r)\n",
    "\n",
    "# soup = BeautifulSoup(r.content, 'html.parser')\n",
    "# print(soup.prettify())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll try a more elaborate approach using rotating headers, proxies and delay to emulate a human based approach. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First step : I scrap a list of proxies and save them in a file, also I use the list of fake user agents to avoid being blocked by the website (source : https://github.com/amaury-joycode/tutoriels/blob/main/python_confirme/scraping_detection/Web%20Scaping%20Python%20-%20%C3%A9viter%20la%20d%C3%A9tection.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import requests\n",
    "\n",
    "# response = requests.get(\"https://free-proxy-list.net/\")\n",
    "\n",
    "# proxy_list = pd.read_html(response.text)[0]\n",
    "# proxy_list[\"url\"] = \"http://\" + proxy_list[\"IP Address\"] + \":\" + proxy_list[\"Port\"].astype(str)\n",
    "# proxy_list.head()\n",
    "\n",
    "# # on copie ici avec pd.DataFrame pour pouvoir ajouter proprement une colonne ensuite\n",
    "# https_proxies = proxy_list[proxy_list[\"Https\"] == \"yes\"]\n",
    "# https_proxies.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import yaml\n",
    "\n",
    "# with open(\"headers.yml\") as f_headers:\n",
    "#     browser_headers = yaml.safe_load(f_headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = \"https://httpbin.org/ip\"\n",
    "# good_proxies = set()\n",
    "# headers = browser_headers[\"Chrome\"]\n",
    "# for proxy_url in https_proxies[\"url\"]:\n",
    "#     proxies = {\n",
    "#         \"http\": proxy_url,\n",
    "#         \"https\": proxy_url,\n",
    "#     }\n",
    "    \n",
    "#     try:\n",
    "#         response = requests.get(url, headers=headers, proxies=proxies, timeout=2)\n",
    "#         good_proxies.add(proxy_url)\n",
    "#         print(f\"Proxy {proxy_url} OK, added to good_proxy list\")\n",
    "#     except Exception:\n",
    "#         pass\n",
    "    \n",
    "#     if len(good_proxies) >= 30:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = \"https://httpbin.org/anything\"\n",
    "# for browser, headers in browser_headers.items():\n",
    "#     print(f\"\\n\\nUsing {browser} headers\\n\")\n",
    "#     for proxy_url in good_proxies:\n",
    "#         proxies = proxies = {\n",
    "#             \"http\": proxy_url,\n",
    "#             \"https\": proxy_url,\n",
    "#         }\n",
    "#         try:\n",
    "#             response = requests.get(url, headers=headers, proxies=proxies, timeout=2)\n",
    "#             print(response.json())\n",
    "#         except Exception:\n",
    "#             print(f\"Proxy {proxy_url} failed, trying another one\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "# import yaml\n",
    "\n",
    "# url = \"https://www.legifrance.gouv.fr/acco/id/ACCOTEXT000038514173?dateSignature=&init=true&page=1&query=salaires&searchField=ALL&tab_selection=acco\"\n",
    "# for browser, headers in browser_headers.items():\n",
    "#     print(f\"\\n\\nUsing {browser} headers\\n\")\n",
    "#     for proxy_url in good_proxies:\n",
    "#         proxies = proxies = {\n",
    "#             \"http\": proxy_url,\n",
    "#             \"https\": proxy_url,\n",
    "#         }\n",
    "#         try:\n",
    "#             response = requests.get(url, headers=headers, proxies=proxies, timeout=2)\n",
    "#             print(response.json())\n",
    "#         except Exception:\n",
    "#             print(f\"Proxy {proxy_url} failed, trying another one\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still a failure. I will try to use Selenium to open the page and then get the HTML code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from selenium import webdriver\n",
    "\n",
    "# for proxy_url in good_proxies:\n",
    "#     proxy = proxy_url.replace(\"http://\", \"\")\n",
    "\n",
    "#     firefox_capabilities = webdriver.DesiredCapabilities.FIREFOX\n",
    "#     firefox_capabilities['marionette'] = True\n",
    "\n",
    "#     firefox_capabilities['proxy'] = {\n",
    "#         \"proxyType\": \"MANUAL\",\n",
    "#         \"httpProxy\": proxy,\n",
    "#         \"sslProxy\": proxy\n",
    "#     }\n",
    "\n",
    "#     driver = webdriver.Firefox(capabilities=firefox_capabilities)\n",
    "#     try:\n",
    "#         driver.get(\"https://www.legifrance.gouv.fr/acco/id/ACCOTEXT000038514173?dateSignature=&init=true&page=1&query=salaires&searchField=ALL&tab_selection=acco\")\n",
    "#     except Exception:\n",
    "#         pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import yaml\n",
    "# from selenium import webdriver\n",
    "\n",
    "# with open(\"headers.yml\") as f_headers:\n",
    "#     browser_headers = yaml.safe_load(f_headers)\n",
    "\n",
    "# good_proxies = set()\n",
    "# headers = browser_headers[\"Chrome\"]\n",
    "    \n",
    "# for proxy_url in good_proxies:\n",
    "#     proxy = proxy_url.replace(\"http://\", \"\")\n",
    "\n",
    "#     firefox_capabilities = webdriver.DesiredCapabilities.FIREFOX\n",
    "#     firefox_capabilities['marionette'] = True\n",
    "\n",
    "#     firefox_capabilities['proxy'] = {\n",
    "#         \"proxyType\": \"MANUAL\",\n",
    "#         \"httpProxy\": proxy,\n",
    "#         \"sslProxy\": proxy\n",
    "#     }\n",
    "\n",
    "#     driver = webdriver.Firefox(capabilities=firefox_capabilities)\n",
    "#     try:\n",
    "#         driver.get(\"https://www.legifrance.gouv.fr/acco/id/ACCOTEXT000038514173?dateSignature=&init=true&page=1&query=salaires&searchField=ALL&tab_selection=acco\")\n",
    "#     except Exception:\n",
    "#         pass\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intent with Selenium V1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from selenium import webdriver\n",
    "# import time\n",
    "\n",
    "# options = webdriver.ChromeOptions()\n",
    "# prefs = {\"download.default_directory\": \"/Users/GARSON/Dares-Alpha/\"}\n",
    "# options.add_experimental_option(\"prefs\", prefs)\n",
    "\n",
    "# driver = webdriver.Chrome(executable_path='/Users/GARSON/Dares-Alpha/chromedriver_mac64/chromedriver', options=options)\n",
    "\n",
    "# try:\n",
    "#     driver.get('https://www.legifrance.gouv.fr/acco/id/ACCOTEXT000038514173?dateSignature=&init=true&page=1&query=salaires&searchField=ALL&tab_selection=acco')\n",
    "#     download_doc = driver.find_element_by_class_name('doc-download')\n",
    "#     download_doc.click()\n",
    "#     time.sleep(10)\n",
    "#     driver.quit()\n",
    "# except:\n",
    "#     print('Error')\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intent with Selenium V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from selenium import webdriver\n",
    "# from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "# options = webdriver.ChromeOptions()\n",
    "# options.add_argument('--headless')\n",
    "# options.add_argument(\"--window-size=1920,1200\")\n",
    "\n",
    "\n",
    "# DRIVER_PATH = '/Users/GARSON/Dares-Alpha/chromedriver_mac64/chromedriver'\n",
    "# driver = webdriver.Chrome(options=options, executable_path=DRIVER_PATH)\n",
    "# driver.get(\"https://www.legifrance.gouv.fr/acco/id/ACCOTEXT000038514173?dateSignature=&init=true&page=1&query=salaires&searchField=ALL&tab_selection=acco\")\n",
    "# print(driver.page_source)\n",
    "# driver.quit()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It worrkkkksss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from selenium import webdriver\n",
    "# from selenium.webdriver.chrome.options import Options\n",
    "# from bs4 import BeautifulSoup\n",
    "\n",
    "# options = webdriver.ChromeOptions()\n",
    "# options.add_argument('--headless')\n",
    "# options.add_argument(\"--window-size=1920,1200\")\n",
    "# prefs = {\"download.default_directory\": \"/Users/GARSON/Dares-Alpha/\"}\n",
    "# options.add_experimental_option(\"prefs\", prefs)\n",
    "\n",
    "\n",
    "# DRIVER_PATH = '/Users/GARSON/Dares-Alpha/chromedriver_mac64/chromedriver'\n",
    "# driver = webdriver.Chrome(options=options, executable_path=DRIVER_PATH)\n",
    "# driver.get(\"https://www.legifrance.gouv.fr/acco/id/ACCOTEXT000037057346?dateSignature=&init=true&page=1&query=salaires&searchField=ALL&tab_selection=acco\")\n",
    "# html = driver.page_source\n",
    "\n",
    "# soup = BeautifulSoup(html, 'html.parser')\n",
    "# doc_url = soup.find(\"a\", attrs={\"class\": \"doc-download\"})['href']\n",
    "\n",
    "# url = f\"https://www.legifrance.gouv.fr/{doc_url}\"\n",
    "\n",
    "# driver.get(url)\n",
    "\n",
    "# driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from selenium import webdriver\n",
    "# from selenium.webdriver.chrome.options import Options\n",
    "# from selenium.webdriver.common.by import By\n",
    "\n",
    "# download_dir = '/Users/GARSON/Dares-Alpha/'\n",
    "# os.makedirs(download_dir, exist_ok=True)\n",
    "\n",
    "# options = webdriver.ChromeOptions()\n",
    "# options.add_argument('--headless')\n",
    "# options.add_argument(\"--window-size=1920,1080\")\n",
    "# options.add_experimental_option(\"prefs\", {\n",
    "#     \"download.default_directory\": download_dir,\n",
    "#     \"download.prompt_for_download\": False,\n",
    "#     \"download.directory_upgrade\": True,\n",
    "#     \"safebrowsing.enabled\": True\n",
    "#     })\n",
    "\n",
    "\n",
    "# DRIVER_PATH = '/Users/GARSON/Dares-Alpha/chromedriver_mac64/chromedriver'\n",
    "# driver = webdriver.Chrome(options=options, executable_path=DRIVER_PATH)\n",
    "# driver.get(\"https://www.legifrance.gouv.fr/acco/id/ACCOTEXT000038514173?dateSignature=&init=true&page=1&query=salaires&searchField=ALL&tab_selection=acco\")\n",
    "# driver.find_element(By.CLASS_NAME, \"doc-download\").click()\n",
    "# print(driver.page_source)\n",
    "\n",
    "# driver.quit()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doesn't dowload the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import wget\n",
    "# from selenium import webdriver\n",
    "# from selenium.webdriver.chrome.options import Options\n",
    "# from bs4 import BeautifulSoup\n",
    "# from incapsula import IncapSession\n",
    "# import urllib.request\n",
    "\n",
    "# options = webdriver.ChromeOptions()\n",
    "# options.add_argument('--headless')\n",
    "# options.add_argument(\"--window-size=1920,1200\")\n",
    "\n",
    "# DRIVER_PATH = '/Users/GARSON/Dares-Alpha/chromedriver_mac64/chromedriver'\n",
    "# driver = webdriver.Chrome(options=options, executable_path=DRIVER_PATH)\n",
    "# driver.get(\"https://www.legifrance.gouv.fr/acco/id/ACCOTEXT000038341025?dateSignature=&init=true&page=1&query=salaires&searchField=ALL&tab_selection=acco\")\n",
    "# html = driver.page_source\n",
    "\n",
    "# soup = BeautifulSoup(html, 'html.parser')\n",
    "# for link in soup.findAll('a'):\n",
    "#     print(link.get('href'))\n",
    "\n",
    "# #doc_cell = soup.findAll(\"a\", attrs={\"class\": \"doc-download\"})\n",
    "# #doc_url = doc_cell['href']\n",
    "# #print(doc_url)\n",
    "\n",
    "# driver.quit()\n",
    "\n",
    "# # Download the document\n",
    "# download_url = f\"https://www.legifrance.gouv.fr/{doc_url}\"\n",
    "# #session = IncapSession(user_agent='Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:109.0) Gecko/20100101 Firefox/113.0')\n",
    "# #response = session.get(download_url)\n",
    "# driver.get(download_url)\n",
    "# print(driver.page_source)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final intent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from selenium import webdriver \n",
    "# import time\n",
    "# from bs4 import BeautifulSoup as bs\n",
    " \n",
    "# try:\n",
    " \n",
    "#     options = webdriver.ChromeOptions ()\n",
    " \n",
    "#     prefs = {\"download.default_directory\": \"C:\\\\Users\\\\garsonj\\\\Desktop\\\\NAO_sample\"}\n",
    " \n",
    "#     options.add_experimental_option(\"prefs\",prefs)\n",
    " \n",
    "#     driver = webdriver.Chrome(options=options)\n",
    " \n",
    "#     driver.get('https://www.legifrance.gouv.fr/acco/id/ACCOTEXT000038514173?dateSignature=&init=true&page=1&query=salaires&searchField')\n",
    " \n",
    "#     html = driver.page_source\n",
    " \n",
    "#     soup = bs(html, 'html.parser')\n",
    " \n",
    "#     download_url = 'https://www.legifrance.gouv.fr' + soup.find_all(class_=\"doc-download\")[0].attrs['href']\n",
    " \n",
    "#     driver.get(download_url)\n",
    " \n",
    "#     time.sleep(2)\n",
    " \n",
    "#     driver.quit()\n",
    " \n",
    "# except:\n",
    " \n",
    "#     print('Error')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I want to turn it into a function that I'll iterate over many pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from selenium import webdriver\n",
    "# from bs4 import BeautifulSoup as bs\n",
    "# import time\n",
    "# import glob\n",
    "# import os\n",
    "\n",
    "# with open(\"Liste_URL.txt\", \"r\") as f:\n",
    "#     input_directory = f.readlines()\n",
    "\n",
    "# def download_docx(input_directory):\n",
    "#     try:\n",
    "#         for url in input_directory:\n",
    "\n",
    "#             options = webdriver.ChromeOptions ()\n",
    "    \n",
    "#             options.add_experimental_option(\"prefs\",prefs)\n",
    "    \n",
    "#             driver = webdriver.Chrome(options=options)\n",
    "    \n",
    "#             driver.get(url)\n",
    "    \n",
    "#             html = driver.page_source\n",
    "    \n",
    "#             soup = bs(html, 'html.parser')\n",
    "    \n",
    "#             download_url = 'https://www.legifrance.gouv.fr' + soup.find_all(class_=\"doc-download\")[0].attrs['href']\n",
    "    \n",
    "#             driver.get(download_url)\n",
    "    \n",
    "#             time.sleep(2)\n",
    "    \n",
    "#             driver.quit()\n",
    " \n",
    "#     except:\n",
    " \n",
    "#         print('Error')\n",
    "\n",
    "# download_docx(input_directory)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I want to iterate this action over a series of url link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Now I want ot iterate over a series of urls\n",
    "\n",
    "# import pandas as pd\n",
    "\n",
    "# df = pd.read_excel('/Users/GARSON/Desktop/NAO/Echantillon_pharma_2023.xlsx')\n",
    "# col1 = df['UrlLegifrance']\n",
    "# print(col1.head())\n",
    "# print(col1.count())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try with another column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import time\n",
    "# from selenium import webdriver\n",
    "# from bs4 import BeautifulSoup as bs\n",
    "\n",
    "# def download_url(url, folder_path):\n",
    "#     try: \n",
    "#         options = webdriver.ChromeOptions ()\n",
    "#         prefs = {\"download.default_directory\": folder_path}\n",
    "#         options.add_experimental_option(\"prefs\",prefs)\n",
    "#         options.add_argument('--headless')\n",
    "#         driver = webdriver.Chrome(options=options)\n",
    "#         driver.get(url)\n",
    "#         driver.quit()\n",
    "#     except:\n",
    "#         print('Error')\n",
    "\n",
    "# df = pd.read_excel(\"C:\\\\Users\\\\garsonj\\\\Desktop\\\\NAO_sample\\\\Echantillon_pharma_2023.xlsx\")[0:10]['Fichier']\n",
    "# for Fichier in df:\n",
    "#     download_url(Fichier, \"C:\\\\Users\\\\garsonj\\\\Desktop\\\\NAO_sample\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Final code\n",
    "\n",
    "# import pandas as pd\n",
    "# import requests\n",
    "# from bs4 import BeautifulSoup as bs\n",
    "\n",
    "# response = requests.get('https://www.legifrance.gouv.fr/acco/id/ACCOTEXT000038514173?dateSignature=&init=true&page=1&query=salaires&searchField') # open the link and get the HTML content\n",
    "# soup = bs(response.content, \"html.parser\") # parse the HTML content into a readable format\n",
    "# download_link = soup.findAll(\"a\", attrs={\"class\": \"doc-download\", \"target\": \"_blank\"})\n",
    "# print(soup.prettify())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
