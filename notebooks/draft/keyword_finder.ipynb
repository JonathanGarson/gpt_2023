{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keyword NLP algorithm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keyword collecting information, no NLP"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Dev: Finding word and the related numbers in a text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt1 = \"0.4% d'augmentation générale des salaires. Revalorisation de la prime de déplacement de 4.35% (de 0.23 à 0.24€/km. Augmentation de la prime de panier de 1.33% (de 3.77 à 3.82€).\"\n",
    "\n",
    "def findNumber(txt, keywords):\n",
    "    txt=txt.lower()\n",
    "    posWords = {}\n",
    "    for keyword in keywords:\n",
    "        if keyword in txt:\n",
    "            posWords[keyword] = txt.index(keyword)\n",
    "        else:\n",
    "            posWords[keyword] = 0\n",
    "    dicItem={}\n",
    "    for keyword in keywords:\n",
    "        dicItem[keyword]={\n",
    "            \"dist\":None,\n",
    "            \"number\":None\n",
    "        }\n",
    "    i=0\n",
    "    while i<len(txt):\n",
    "        if txt[i].isdigit():\n",
    "            fullDigit=txt[i]\n",
    "            k=i\n",
    "            while txt[k+1].isdigit():\n",
    "                fullDigit+=txt[k+1]\n",
    "                k+=1\n",
    "            bestDist =1000000\n",
    "            bestKey=\"\"\n",
    "            for key in posWords:\n",
    "                dist=abs(posWords[key]-int(i))\n",
    "                if dist < bestDist:\n",
    "                    bestDist=dist\n",
    "                    bestKey=key\n",
    "                if bestDist < 20:\n",
    "                    if not dicItem[bestKey][\"dist\"] or bestDist < dicItem[bestKey][\"dist\"]:\n",
    "                        dicItem[bestKey][\"dist\"]=bestDist\n",
    "                        dicItem[bestKey][\"number\"]=float(fullDigit)\n",
    "            i=k\n",
    "        i+=1\n",
    "    return dicItem\n",
    "\n",
    "print(findNumber(txt1, [\"générale\", \"valeur\"]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Second Dev**: Detecting specific words and highlighting them "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attempt 1: with a txt file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "left \u001b[44m\u001b[97mfoot\u001b[0m right \u001b[44m\u001b[97mfoot\u001b[0m left \u001b[44m\u001b[97mfoot\u001b[0m right. \u001b[44m\u001b[97mfeet\u001b[0m in the day, \u001b[44m\u001b[97mfeet\u001b[0m at night.\n"
     ]
    }
   ],
   "source": [
    "from termcolor import colored\n",
    "text='left foot right foot left foot right. Feet in the day, feet at night.'\n",
    "l1=['foot','feet']\n",
    "formattedText = []\n",
    "for t in text.lower().split():\n",
    "    if t in l1:\n",
    "        formattedText.append(colored(t,'white','on_blue'))\n",
    "    else: \n",
    "        formattedText.append(t)\n",
    "\n",
    "print(\" \".join(formattedText))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attempt 1.2: Build a function to highlight the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Négociation annuelle obligatoire Procès verbal de la réunion de clôture du 23/05/2018 A la suite des 3 réunions de négociation qui ont eu lieu les 18 avril, 2 mai et 23 mai 2018, il est établi le présent procès verbal d'accord. Cette négociation a eu lieu en présence de la délégation syndicale CFDT représentée par Monsieur accompagné de Monsieur Le tableau relatif aux chiffres permettant une analyse des points suivants n’a pas suscité de commentaire ou revendication particulière de la délégation syndicale. Il permettait une vision de l’état 2017 : - Des salaires et temps de travail dans l’entreprise - De l’emploi - De l’égalité homme – femme - De l’emploi de travailleurs handicapés Les revendications de la délégation CFDT étaient les suivantes : - 1,5% d’augmentation \u001b[44m\u001b[97mgénérale\u001b[0m des salaires. - Revalorisation de la \u001b[44m\u001b[97mprime\u001b[0m de déplacement de 5%. - Augmentation de la \u001b[44m\u001b[97mprime\u001b[0m de panier de 5% avec pour ceux qui ne l'ont pas une \u001b[44m\u001b[97mprime\u001b[0m de 150€. Les propositions de la direction ont été les suivantes : Malgré une année 2017 très difficile et une situation sur début 2018 préoccupante qui ont pour conséquences une persistance des résultats négatifs, la direction propose : - 0.4% d'augmentation \u001b[44m\u001b[97mgénérale\u001b[0m des salaires. - Revalorisation de la \u001b[44m\u001b[97mprime\u001b[0m de déplacement de 4.35% (de 0.23 à 0.24€/km). - Augmentation de la \u001b[44m\u001b[97mprime\u001b[0m de panier de 1.33% (de 3.77 à 3.82€). Lors de la troisième réunion, les résultats de la négociation ont été les suivants : - Le faible niveau du carnet de commandes de ce début d'année a conduit à un recours massif d'activité partielle sur les 4 premiers mois de 2018 et la situation de l’entreprise reste fragile malgré une amélioration du carnet de commandes depuis fin avril. Cependant, malgré ces conditions et après discussion, l'entreprise propose : o 0.5% d'augmentation \u001b[44m\u001b[97mgénérale\u001b[0m des salaires. o Revalorisation de la \u001b[44m\u001b[97mprime\u001b[0m de déplacement de 4.35% (de 0.23 à 0.24€/km) limitée à une distance maximale de 50 km. o Augmentation de la \u001b[44m\u001b[97mprime\u001b[0m de panier de 1.33% (de 3.77 à 3.82€). - Concernant les autres demandes de la délégation : o La \u001b[44m\u001b[97mprime\u001b[0m de panier est liée à une contrainte supportée par les salariés travaillant en double équipe ; il n'y a donc pas lieu d'octroyer une \u001b[44m\u001b[97mprime\u001b[0m de compensation aux salariés qui ne sont pas soumis à cette contrainte. Fait à Dammarie Sur Saulx le 23 mai 2018. Pour la direction Pour la délégation syndicale Le Délégué Syndical\n"
     ]
    }
   ],
   "source": [
    "from termcolor import colored\n",
    "\n",
    "def color(file_path, keywords):\n",
    "    text=open(file_path,'r').read().split()\n",
    "    formattedText = []\n",
    "    for t in text:\n",
    "        if t in keywords:\n",
    "            formattedText.append(colored(t,'white','on_blue'))\n",
    "        else: \n",
    "            formattedText.append(t)\n",
    "    print(\" \".join(formattedText))\n",
    "\n",
    "keywords=['augmentation', 'générale', 'prime', 'augmentations', 'générales', 'primes', 'individuelle']\n",
    "color('test.txt', keywords)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attempt 1.3 : Implementation of a normalization process in the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "négociation annuelle obligatoire procès verbal de la réunion de clôture du 23/05/2018 a la suite des 3 réunions de négociation qui ont eu lieu les 18 avril, 2 mai et 23 mai 2018, il est établi le présent procès verbal d accord. cette négociation a eu lieu en présence de la délégation syndicale cfdt représentée par monsieur accompagné de monsieur le tableau relatif aux chiffres permettant une analyse des points suivants n’a pas suscité de commentaire ou revendication particulière de la délégation syndicale. il permettait une vision de l’état 2017 :   des salaires et temps de travail dans l’entreprise   de l’emploi   de l’égalité homme – femme   de l’emploi de travailleurs handicapés les revendications de la délégation cfdt étaient les suivantes :   1,5% d’augmentation \u001b[44m\u001b[97mgénérale\u001b[0m des salaires.   revalorisation de la \u001b[44m\u001b[97mprime\u001b[0m de déplacement de 5%.   \u001b[44m\u001b[97maugmentation\u001b[0m de la \u001b[44m\u001b[97mprime\u001b[0m de panier de 5% avec pour ceux qui ne l ont pas une \u001b[44m\u001b[97mprime\u001b[0m de 150€. les propositions de la direction ont été les suivantes : malgré une année 2017 très difficile et une situation sur début 2018 préoccupante qui ont pour conséquences une persistance des résultats négatifs, la direction propose :   0.4% d augmentation \u001b[44m\u001b[97mgénérale\u001b[0m des salaires.   revalorisation de la \u001b[44m\u001b[97mprime\u001b[0m de déplacement de 4.35% (de 0.23 à 0.24€/km).   \u001b[44m\u001b[97maugmentation\u001b[0m de la \u001b[44m\u001b[97mprime\u001b[0m de panier de 1.33% (de 3.77 à 3.82€). lors de la troisième réunion, les résultats de la négociation ont été les suivants :   le faible niveau du carnet de commandes de ce début d année a conduit à un recours massif d activité partielle sur les 4 premiers mois de 2018 et la situation de l’entreprise reste fragile malgré une amélioration du carnet de commandes depuis fin avril. cependant, malgré ces conditions et après discussion, l entreprise propose : o 0.5% d augmentation \u001b[44m\u001b[97mgénérale\u001b[0m des salaires. o revalorisation de la \u001b[44m\u001b[97mprime\u001b[0m de déplacement de 4.35% (de 0.23 à 0.24€/km) limitée à une distance maximale de 50 km. o \u001b[44m\u001b[97maugmentation\u001b[0m de la \u001b[44m\u001b[97mprime\u001b[0m de panier de 1.33% (de 3.77 à 3.82€).   concernant les autres demandes de la délégation : o la \u001b[44m\u001b[97mprime\u001b[0m de panier est liée à une contrainte supportée par les salariés travaillant en double équipe ; il n y a donc pas lieu d octroyer une \u001b[44m\u001b[97mprime\u001b[0m de compensation aux salariés qui ne sont pas soumis à cette contrainte. fait à dammarie sur saulx le 23 mai 2018. pour la direction pour la délégation syndicale le délégué syndical\n"
     ]
    }
   ],
   "source": [
    "from termcolor import colored\n",
    "import os \n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_excel('keywords.xlsx')\n",
    "keywords = df['keywords'].tolist()\n",
    "\n",
    "remove = {\"'\":\" \", \"-\":\" \", \"«\":\" \", \"»\":\" \", \"“\":\" \", \"”\":\" \"}\n",
    "\n",
    "def color(file_path, keywords):\n",
    "    text0=open(file_path,'r').read().split()\n",
    "    text = [t.lower() for t in text0]\n",
    "    for key in remove: \n",
    "        text = [t.replace(key, remove[key]) for t in text]\n",
    "    formattedText = []\n",
    "    for t in text:\n",
    "        if t in keywords:\n",
    "            formattedText.append(colored(t,'white','on_blue'))\n",
    "        else: \n",
    "            formattedText.append(t)\n",
    "    print(\" \".join(formattedText))\n",
    "\n",
    "color('test.txt', keywords)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is working and highlights the import part of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test.txt', 'r') as r:\n",
    "    read = r.read()\n",
    "    formatted_text = []\n",
    "    for t in read:\n",
    "        if t in keywords:\n",
    "            formatted_text.append(colored(t,'white','on_blue'))\n",
    "        else: \n",
    "            formatted_text.append(t)\n",
    "\n",
    "with open('test.txt', 'w') as w:\n",
    "    for i in formatted_text:\n",
    "        w.write(i)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Attempt 1.4**: Now I want to export the change to a new file. **It works**. However, to be optimal it needs a specific text editor or another format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from termcolor import colored\n",
    "import os \n",
    "import pandas as pd\n",
    "\n",
    "# import list of keywords\n",
    "df = pd.read_excel('keywords.xlsx')\n",
    "keywords = df['keywords'].tolist()\n",
    "\n",
    "\n",
    "\n",
    "#text cleaning\n",
    "\n",
    "def clean(file_path):\n",
    "    with open(file_path, \"r\") as reader:\n",
    "        file_path = reader.read()\n",
    "        removenewline = [t.replace(\"\\n\", \" \") for t in file_path]\n",
    "        removemaj = [t.lower() for t in removenewline]\n",
    "        remove = { \".\": \" \",\"'\":\" \", \"-\":\" \", \"«\":\" \", \"»\":\" \", \"“\":\" \", \"”\":\" \"}\n",
    "        for key in remove:\n",
    "            removemaj = [t.replace(key, remove[key]) for t in removemaj]\n",
    "    with open(file_path, \"w\") as writer:\n",
    "        for i in removemaj:\n",
    "            writer.write(i)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from termcolor import colored\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_excel('keywords.xlsx')\n",
    "keywords = df['keywords'].tolist()\n",
    "\n",
    "# Now we write as a function\n",
    "\n",
    "def clean(file_path):\n",
    "    new = []\n",
    "    with open(file_path, \"r\") as reader:\n",
    "        for line in reader:\n",
    "            new.append(line)\n",
    "    removeMaj = []\n",
    "    removeNewline = []\n",
    "    clean_txt = []\n",
    "    for i in new:\n",
    "        a = i.lower()\n",
    "        removeMaj.append(a)\n",
    "\n",
    "    for i in removeMaj:\n",
    "        b = i.replace(\"\\n\", \" \")\n",
    "        removeNewline.append(b)\n",
    "\n",
    "    for i in removeNewline:\n",
    "        c = i.replace(\"'\", \" \")\n",
    "        c = c.replace(\"-\", \" \")\n",
    "        c = c.replace(\"«\", \" \")\n",
    "        c = c.replace(\"»\", \" \")\n",
    "        c = c.replace(\"“\", \" \")\n",
    "        c = c.replace(\"”\", \" \")\n",
    "        c = c.replace(\":\", \" \")\n",
    "        clean_txt.append(c)\n",
    "\n",
    "    with open(file_path, \"w\") as writer:\n",
    "        for i in clean_txt:\n",
    "            writer.write(i)\n",
    "\n",
    "def color_text_file(file_path, keywords):\n",
    "    with open(file_path, 'r') as r:\n",
    "        content = r.read().split()\n",
    "\n",
    "    formatted_text = []\n",
    "    for word in content:\n",
    "        if word in keywords:\n",
    "            formatted_text.append(colored(word, 'white', 'on_blue'))\n",
    "        else: \n",
    "            formatted_text.append(word)\n",
    "\n",
    "    with open(file_path, 'w') as w:\n",
    "        for formatted_word in formatted_text:\n",
    "            w.write(formatted_word + ' ')\n",
    "\n",
    "clean(r'.\\sample.txt\\T05518000047-41936943400016.txt')\n",
    "color_text_file(r'.\\sample.txt\\T05518000047-41936943400016.txt', keywords)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attempt 2: with a word file"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Attempt 2.1 : it works.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx import Document\n",
    "from docx.enum.text import WD_COLOR\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "list_of_words = ['salaires']\n",
    "\n",
    "# Setup regex\n",
    "patterns = [r'\\b' + word + r'\\b' for word in list_of_words]\n",
    "re_highlight = re.compile('(' + '|'.join(p for p in patterns) + ')+',\n",
    "                          re.IGNORECASE)\n",
    "\n",
    "file = r\".\\sample.docx\\T05518000047-41936943400016.docx\"\n",
    "doc = Document(file)\n",
    "for para in doc.paragraphs:\n",
    "    text = para.text\n",
    "    if len(re_highlight.findall(text)) > 0:\n",
    "        matches = re_highlight.finditer(text)\n",
    "        para.text = ''\n",
    "        p3 = 0\n",
    "        for match in matches:\n",
    "            p1 = p3\n",
    "            p2, p3 = match.span()\n",
    "            para.add_run(text[p1:p2])\n",
    "            run = para.add_run(text[p2:p3])\n",
    "            run.font.highlight_color = WD_COLOR.YELLOW\n",
    "            para.add_run(text[p3:])\n",
    "doc.save(file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Attempt 2.2** : implementing a generalisation of the list of words (works) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx import Document\n",
    "from docx.enum.text import WD_COLOR\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "df = pd.read_excel('keywords.xlsx')\n",
    "list_of_words = df['keywords'].tolist()\n",
    "\n",
    "# Setup regex\n",
    "patterns = [r'\\b' + word + r'\\b' for word in list_of_words]\n",
    "re_highlight = re.compile('(' + '|'.join(p for p in patterns) + ')+',\n",
    "                          re.IGNORECASE)\n",
    "\n",
    "file = r\".\\sample.docx\\T05518000047-41936943400016.docx\"\n",
    "doc = Document(file)\n",
    "for para in doc.paragraphs:\n",
    "    text = para.text\n",
    "    if len(re_highlight.findall(text)) > 0:\n",
    "        matches = re_highlight.finditer(text)\n",
    "        para.text = ''\n",
    "        p3 = 0\n",
    "        for match in matches:\n",
    "            p1 = p3\n",
    "            p2, p3 = match.span()\n",
    "            para.add_run(text[p1:p2])\n",
    "            run = para.add_run(text[p2:p3])\n",
    "            run.font.highlight_color = WD_COLOR.YELLOW\n",
    "            para.add_run(text[p3:])\n",
    "doc.save(file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Attempt 2.3**: doing iteratevely over a folder of docx document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx import Document\n",
    "from docx.enum.text import WD_COLOR\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "\n",
    "df = pd.read_excel('keywords.xlsx')\n",
    "list_of_words = df['keywords'].tolist()\n",
    "\n",
    "# Setup regex\n",
    "patterns = [r'\\b' + word + r'\\b' for word in list_of_words]\n",
    "re_highlight = re.compile('(' + '|'.join(p for p in patterns) + ')+',\n",
    "                          re.IGNORECASE)\n",
    "\n",
    "folder_path = r'.\\sample.docx'  # Replace with the actual folder path where your DOCX files are located\n",
    "\n",
    "docx_files = glob.glob(folder_path + '\\*.docx')\n",
    "for file in docx_files:\n",
    "    doc = Document(file)\n",
    "    for para in doc.paragraphs:\n",
    "        text = para.text\n",
    "        if len(re_highlight.findall(text)) > 0:\n",
    "            matches = re_highlight.finditer(text)\n",
    "            para.text = ''\n",
    "            p3 = 0\n",
    "            for match in matches:\n",
    "                p1 = p3\n",
    "                p2, p3 = match.span()\n",
    "                para.add_run(text[p1:p2])\n",
    "                run = para.add_run(text[p2:p3])\n",
    "                run.font.highlight_color = WD_COLOR.YELLOW\n",
    "                para.add_run(text[p3:])\n",
    "    doc.save(file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Attempt 2.3 : IT WORKS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx import Document\n",
    "from docx.enum.text import WD_COLOR\n",
    "import pandas as pd\n",
    "import glob\n",
    "import re\n",
    "\n",
    "# import list of keywords\n",
    "df = pd.read_excel('keywords.xlsx')\n",
    "list_of_words = df['keywords'].tolist()\n",
    "\n",
    "# Setup regex\n",
    "patterns = [r'\\b' + word + r'\\b' for word in list_of_words]\n",
    "re_highlight = re.compile('(' + '|'.join(p for p in patterns) + ')+', re.IGNORECASE)\n",
    "\n",
    "# set folder path\n",
    "folder_path = '.\\\\sample.docx' \n",
    "docx_files = glob.glob(folder_path + '\\\\*.docx')\n",
    "\n",
    "# loop through all files in folder\n",
    "for file in docx_files:\n",
    "    doc = Document(file)\n",
    "    for para in doc.paragraphs:\n",
    "        text = para.text\n",
    "        if len(re_highlight.findall(text)) > 0:\n",
    "            matches = re_highlight.finditer(text)\n",
    "            para.text = ''\n",
    "            p3 = 0\n",
    "            for match in matches:\n",
    "                p1 = p3\n",
    "                p2, p3 = match.span()\n",
    "                para.add_run(text[p1:p2])\n",
    "                run = para.add_run(text[p2:p3])\n",
    "                run.font.highlight_color = WD_COLOR.YELLOW\n",
    "                para.add_run(text[p3:])\n",
    "    doc.save(file)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I'll make an exe code out of it."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keyword collection with NLP"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Attempt 1.1 :** using a basic sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import csv\n",
    "\n",
    "def extract_number(sentence):\n",
    "    # Text preprocessing\n",
    "    sentence = re.sub(r'[^\\w\\s]', '', sentence)\n",
    "\n",
    "    # Tokenization\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "\n",
    "    # Number extraction\n",
    "    number = None\n",
    "    for token in tokens:\n",
    "        if re.match(r'\\d+', token):\n",
    "            number = int(token)\n",
    "            break\n",
    "\n",
    "    # Element identification\n",
    "    element = 'speed'  # Placeholder; you can implement more advanced logic here\n",
    "\n",
    "    return number, element\n",
    "\n",
    "def export_to_csv(number, element):\n",
    "    with open('data.csv', 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(['Number', 'Element'])\n",
    "        writer.writerow([number, element])\n",
    "\n",
    "# Example usage\n",
    "sentence = \"There are 4 trains going at 200km/h\"\n",
    "number, element = extract_number(sentence)\n",
    "export_to_csv(number, element)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Attempt 2:** changing the word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re as re_module\n",
    "import nltk\n",
    "import csv\n",
    "                 \n",
    "def extract_number(sentence):\n",
    "    # Text preprocessing\n",
    "    sentence = re_module.sub(r'[^\\w\\s]', '', sentence)\n",
    "\n",
    "    # Tokenization\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "\n",
    "    # Number extraction\n",
    "    number = None\n",
    "    for token in tokens:\n",
    "        if re_module.match(r'(\\d+[\\.]?\\d+?%)', token) and re_module.match(r'(\\d+[\\,]?\\d+?%)', token) and token not in {'2022', '2023'} and not re_module.match(r'\\d{1,2}/\\d{1,2}/\\d{2,4}', token) and not re_module.match(r'([123]\\d\\d\\d)', token) and not re_module.match(r\"\\b(\\d{1,2}\\s+(?:janvier|février|mars|avril|mai|juin|juillet|août|septembre|octobre|novembre|décembre)\\s+\\d{4})\\b\", token):\n",
    "            number = float(token)\n",
    "            break\n",
    "\n",
    "    # Element identification\n",
    "    element = 'augmentation'  # Placeholder; you can implement more advanced logic here\n",
    "\n",
    "    return number, element\n",
    "\n",
    "def export_to_csv(number, element):\n",
    "    with open('t1.csv', 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(['Number', 'Element'])\n",
    "        writer.writerow([number, element])\n",
    "\n",
    "# Example usage\n",
    "sentence = \"Les revendications de la délégation CFDT étaient les suivantes: 1,5% 'augmentation générale des salaires.\"\n",
    "number, element = extract_number(sentence)\n",
    "export_to_csv(number, element)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Attempt 2.3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import re\n",
    "\n",
    "def extract_numerical_values(text, keywords):\n",
    "    numerical_values = []\n",
    "\n",
    "    # Tokenize the text into words\n",
    "    tokenizer = RegexpTokenizer(r'\\s+', gaps=True)\n",
    "    tokens = tokenizer.tokenize(text.lower())\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('french'))\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "    # Check for numerical values near the keywords\n",
    "    for i in range(len(filtered_tokens)):\n",
    "        if filtered_tokens[i] in keywords:\n",
    "            # Retrieve the sentence before and after the keyword\n",
    "            prev_sentence = ' '.join(filtered_tokens[:i])\n",
    "            next_sentence = ' '.join(filtered_tokens[i+1:])\n",
    "\n",
    "            # Exclude cases where numerical value is near unwanted words\n",
    "            if 'cgt' not in prev_sentence and 'direction' not in prev_sentence:\n",
    "                # Use regular expressions to match numerical values in both sentences\n",
    "                match_prev = re.search(r'(\\d+(\\.\\d+)?)', prev_sentence[::-1])  # Reverse the sentence for matching from the end\n",
    "                match_next = re.search(r'(\\d+(\\.\\d+)?)', next_sentence)\n",
    "                \n",
    "                if match_prev:\n",
    "                    numerical_values.append(match_prev.group(0)[::-1])  # Reverse the matched value to restore original order\n",
    "                \n",
    "                if match_next:\n",
    "                    numerical_values.append(match_next.group(0))\n",
    "\n",
    "    return numerical_values\n",
    "\n",
    "\n",
    "def store_to_excel(numerical_values):\n",
    "    df = pd.DataFrame({'AG': numerical_values})\n",
    "    df.to_excel('output.xlsx', index=False)\n",
    "\n",
    "\n",
    "text = \"augmentation générale de salaire sera de 1.5% à partir de 1 janvier 2023. la prime au mérite sera de 0.5% et les augmentations individuelles seront de 2%.\"\n",
    "keywords = [\"générale\", \"individuelles\"]\n",
    "\n",
    "numerical_values = extract_numerical_values(text, keywords)\n",
    "store_to_excel(numerical_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['accord', 'd’entreprise', 'relatif', 'négociation', 'annuelle', 'obligatoire', 'rémunération,', 'temps', 'travail', 'partage', 'valeur', 'ajoutée', 'l’année', '2023', 'entre', 'société', ':', 'chugai', 'pharma', 'france', 'sas,', 'société', 'anonyme', 'simplifiée,', 'immatriculée', 'registre', 'commerce', 'sociétés', 'nanterre', 'sous', 'numéro', 'b', '435', '074', '422,', 'capital', '1.000.000', 'd’euros', 'dont', 'siège', 'social', 'situé', 'tour', 'pacific,', '11', 'cour', 'valmy,', '92800', 'puteaux', 'représentée', 'xxx', 'agissant', 'qualité', 'président', 'd’une', 'part,', 'l’organisation', 'syndicale', 'unsa', 'représentative', 'société,', 'représentée', 'xxx,', 'délégué', 'syndical,', 'dûment', 'habilité,', 'd’autre', 'part,', 'préambule', '15', 'novembre', '2022,', 'parties', 'convenues', 'calendrier', 'modalités', 'tenue', 'réunions', 'ainsi', 'informations', 'remettre', 'l’organisation', 'syndicale', 'représentative', 'société.', 'cours', 'réunions', '25', 'novembre', '2022,', '15', 'décembre', '2022', '10', 'janvier', '2022,', 'parties', 'échangé', 'thèmes', 'prévus', 'articles', 'l.', '2242-1', '1°', 'suivants', 'code', 'travail,', 'savoir', 'rémunération,', 'temps', 'travail', 'partage', 'valeur', 'ajoutée', \"l'entreprise.\", 'ici', 'rappelé', 'thèmes', '«', 'temps', 'travail', '»,', '«', 'partage', 'valeur', 'ajoutée', '»', '«', 'égalité', 'professionnelle', '»', 'font', 'l’objet', 'd’accords', 'collectifs', 'd’entreprise', 'spécifiques,', 'vigueur.', 'l’organisation', 'syndicale', 'a', 'communiqué', 'propositions,', 'auxquelles', 'direction', 'a', 'répondu.', 'etat', 'dernières', 'propositions', 'l’unsa', ':', '-', 'rémunérations', ':', 'augmentation', 'générale', '6', '%', '-', 'prime', 'partage', 'valeur', '(ppv):', 'demande', 'd’une', 'prime', '3000', '€', 'salariés', 'figurant', 'l’effectif', 'société', '-', 'épargne', 'salariale', ':', 'demande', \"d'abondement\", 'perco,', 'hauteur', '10', '%', '-', 'rp,', 'we', 'jours', 'travaillés', ':', 'demande', 'revalorisation', '10', '€', 'rp', 'soirs', 'samedis', 'travaillés', '20', '€', 'dimanches', 'jours', 'fériés', 'travaillés', 'a', 'l’issue', 'négociation,', 'a', 'convenu', 'arrêté', 'suit', ':', 'article', '1', '-', 'champ', 'd’application.', 'présent', 'accord', 'concerne', 'l’ensemble', 'salariés', 'société.', 'article', '2', '-', 'durée.', 'présent', 'accord', 'conclu', 'durée', 'indéterminée', 'compter', 'signature.', 'article', '3', '–', 'dispositions', 'relatives', 'rémunération', '3.1', '-', 'égalité', 'professionnelle', 'entre', 'hommes', 'femmes.', 'cours', 'négociations,', 'parties', 'discuté', 'l’application', 'principe', 'd’égalité', 'professionnelle', 'entre', 'femmes', 'hommes', 'sein', 'l’entreprise.', 'a', 'rappelé', 'mesures', 'spécifiques', 'relatives', 'l’égalité', 'rémunération', 'entre', 'hommes', 'femmes', 'convenues', 'termes', 'l’accord', 'collectif', 'd’entreprise', 'relatif', 'l’egalité', 'professionnelle', 'h/f', 'conclu', '1er', 'mai', '2022', 'durée', '4', 'ans.', 'a', 'notamment', 'constaté', 'taux', \"l'index\", 'egalité', 'professionnelle', \"s'élève\", '89/100', 'l’année', '2021.', 'parties', 'conclu', 'qu’il', 'n’y', 'mesures', 'supplémentaires', 'prévoir', 'cadre', 'présente', 'nao.', '3.2', '-', 'augmentations', 'annuelles', 'salaires', 'taux', 'moyen', 'augmentations', 'annuelles', 'individuelles', 'effectif', '1er', 'janvier', '2023', 'décidé', 'direction,', 'appliqué', 'salaires', 'base,', '4.5', '%', 'hors', 'promotion.', 'taux', 'comprend', 'titre', 'exceptionnel,', 'augmentation', 'minimum', 'garanti', '2%', 'appliquée', 'salaires', 'base', 'l’ensemble', 'salariés,', 'effet', '1er', 'janvier', '2023.', '3.3', '–', 'indemnisation', 'rp,', 'week-end', 'jours', 'fériés', 'rp', 'collaborateurs', 'concernés', 'revalorisés', 'compter', '1er', 'janvier', '2023.', 'montants', 'applicables', 'suivants', ':', '-', 'rp', 'soir', ':', '130', '€', 'bruts', '(au', 'lieu', '120', '€', 'bruts)', '-', 'samedi', 'travaillé', ':', '170', '€', 'bruts', '(au', 'lieu', '160', '€', 'bruts)', '-', 'dimanche', 'jour', 'férié', 'travaillé', ':', '200', '€', 'bruts', '(au', 'lieu', '180', '€', 'bruts)', 'article', '4', '–', 'budget', 'activités', 'sociales', 'culturelles', 'comité', 'social', 'économique', 'rappel,', 'revalorisation', 'budget', 'activités', 'sociales', 'culturelles', 'comité', 'social', 'économique', 'appliquée', 'conformément', 'l’accord', 'mise', 'place', 'cse', 'signé', '15', 'septembre', '2022.', 'taux', 'porté', '0.74', '%', '(au', 'lieu', '0.68%)', 'masse', 'salariale', 'annuelle', 'brute', 'telle', 'définie', 'l’article', 'l2312-83', 'code', 'travail', 'compter', '1er', 'janvier', '2023.', 'article', '5', '–', 'prime', 'partage', 'valeur', 'rappel,', 'décision', 'unilatérale', 'direction', 'date', '21', 'décembre', '2022,', 'prime', 'partage', 'valeur', 'versée', 'janvier', '2023.', 'montant', 'prime', 'modulé', 'selon', 'conditions', 'critères', 'cumulatifs', 'définis', 'ladite', 'décision.', 'article', '6', '–', 'révision', 'dénonciation', '6.1', 'révision', 'toute', 'demande', 'révision', 'devra', 'être', 'adressée', 'lettre', 'recommandée', 'accusé', 'réception', 'chacune', 'autres', 'parties', 'signataires', 'adhérentes', 'comporter,', 'outre', 'l’indication', 'dispositions', 'dont', 'révision', 'demandée,', 'propositions', 'remplacement.', 'plus', 'rapidement', 'possible', 'plus', 'tard', 'délai', 'trois', '(3)', 'mois', 'suivant', 'réception', 'cette', 'lettre,', 'parties', 'sus', 'indiquées', 'devront', 'ouvrir', 'négociation', 'vue', 'rédaction', 'éventuelle', 'd’un', 'nouveau', 'texte.', 'dispositions', 'l’accord', 'dont', 'révision', 'demandée', 'resteront', 'vigueur', 'jusqu’à', 'conclusion', 'd’un', 'nouvel', 'accord.', 'dispositions', 'l’avenant', 'portant', 'révision,', 'substitueront', 'plein', 'droit', 'celles', 'l’accord', 'qu’elles', 'modifient', 'opposables', 'l’entreprise', 'salariés', 'liés', 'l’accord,', 'date', 'expressément', 'convenue,', 'défaut', 'partir', 'jour', 'suivra', 'dépôt.', '6.2', 'dénonciation', 'présent', 'accord', 'pourra', 'être', 'dénoncé', \"l'une\", \"l'autre\", 'parties', 'signataires', 'respectant', 'préavis', '3', 'mois.', 'partie', 'dénonce', \"l'accord\", 'doit', 'notifier', 'cette', 'décision,', 'lettre', 'recommandée', 'accusé', 'réception,', 'drieets', 'lettre', 'recommandée', 'accusé', 'réception', 'autres', 'parties', 'signataires.', 'article', '8', '–', 'publicité', 'dépôt', 'présent', 'accord', 'notifié,', 'remise', 'main', 'propre', 'contre', 'décharge', 'auprès', 'délégué', 'syndical,', \"l'organisation\", 'syndicale', 'représentative', 'société.', 'porté', 'connaissance', 'personnel', 'affichage', 'version', 'électronique.', 'présent', 'accord', 'déposé', 'deux', 'exemplaires', '(une', 'version', 'support', 'électronique', 'format', 'pdf', 'version', 'anonymisée', 'support', 'électronique', 'format', '.docx,', 'plateforme', 'téléprocédure', 'ministère', 'travail', 'www.teleaccords.travail-emploi.gouv.fr)', 'exemplaire', '(version', 'papier', 'signée', 'parties)', 'greffe', 'conseil', 'prud’hommes', 'nanterre.', 'fait', 'paris', 'défense,', '11', 'janvier', '2023,', 'direction', ':', 'xxx', 'président', 'l’organisation', 'syndicale', ':', 'l’organisation', 'syndicale', 'unsa,', 'représentée', 'xxx,', 'délégué', 'syndical.', '2']\n"
     ]
    }
   ],
   "source": [
    "def clean(text):\n",
    "\n",
    "    # Tokenize the text into words\n",
    "    tokenizer = RegexpTokenizer(r'\\s+', gaps=True)\n",
    "    tokens = tokenizer.tokenize(text.lower())\n",
    "\n",
    "    # Modify stopwords list\n",
    "    result = set(stopwords.words('french'))\n",
    "    stop_words = set(stopwords.words('french')) - set(['.'])\n",
    "   \n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('french'))\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    return filtered_tokens\n",
    "\n",
    "# Read the text from the file\n",
    "with open('test.txt', 'r') as file:\n",
    "    text = file.read()\n",
    "\n",
    "# Clean the text\n",
    "cleaned_text = clean(text)\n",
    "\n",
    "# Print the cleaned tokens\n",
    "print(cleaned_text)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attempt 2.4: adding conditions of exclusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import re\n",
    "\n",
    "def extract_numerical_values(text, keywords):\n",
    "    numerical_values = []\n",
    "\n",
    "    # Tokenize the text into words\n",
    "    tokenizer = RegexpTokenizer(r'\\s+', gaps=True)\n",
    "    tokens = tokenizer.tokenize(text.lower())\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('french'))\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "    # Check for numerical values near the keywords\n",
    "    for i in range(len(filtered_tokens)):\n",
    "        if filtered_tokens[i] in keywords:\n",
    "            # Retrieve the sentence before and after the keyword\n",
    "            prev_sentence = ' '.join(filtered_tokens[:i])\n",
    "            next_sentence = ' '.join(filtered_tokens[i+1:])\n",
    "\n",
    "            # Exclude cases where numerical value is near unwanted words\n",
    "            if 'cgt' or 'cfdt' or 'cfe-cgc' or 'fo' or 'force' or 'ouvrière' or 'unsa' not in prev_sentence and 'direction' not in prev_sentence:\n",
    "                # Use regular expressions to match numerical values in both sentences\n",
    "                match_prev = re.search(r'(\\d+(\\.\\d+)?)', prev_sentence[::-1])  # Reverse the sentence for matching from the end\n",
    "                match_next = re.search(r'(\\d+(\\.\\d+)?)', next_sentence) \n",
    "                \n",
    "                if match_prev:\n",
    "                    numerical_values.append(match_prev.group(0)[::-1])  # Reverse the matched value to restore original order\n",
    "                \n",
    "                if match_next:\n",
    "                    numerical_values.append(match_next.group(0))\n",
    "\n",
    "    return numerical_values\n",
    "\n",
    "\n",
    "def store_to_excel(numerical_values):\n",
    "    df = pd.DataFrame({'AG': numerical_values})\n",
    "    df.to_excel('output.xlsx', index=False)\n",
    "\n",
    "\n",
    "text = \"La CGT a demandé une augmentation générale de 3% et la direction a proposé 1%. Finalement une augmentation générale de salaire sera de 1.5% à partir de 1 janvier 2023. la prime au mérite sera de 0.5% et les augmentations individuelles seront de 2%.\"\n",
    "keywords = [\"générale\", \"individuelles\"]\n",
    "\n",
    "numerical_values = extract_numerical_values(text, keywords)\n",
    "store_to_excel(numerical_values)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attempt 2.5: trying on real a txt file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import re\n",
    "\n",
    "def extract_numerical_values(text, keywords):\n",
    "    numerical_values = []\n",
    "\n",
    "    # Tokenize the text into words\n",
    "    tokenizer = RegexpTokenizer(r'\\s+', gaps=True)\n",
    "    tokens = tokenizer.tokenize(text.lower())\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('french'))\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "    # Check for numerical values near the keywords\n",
    "    for i in range(len(filtered_tokens)):\n",
    "        if filtered_tokens[i] in keywords:\n",
    "            # Retrieve the sentence before and after the keyword\n",
    "            prev_sentence = ' '.join(filtered_tokens[:i])\n",
    "            next_sentence = ' '.join(filtered_tokens[i+1:])\n",
    "\n",
    "            # Exclude cases where numerical value is near unwanted words\n",
    "            if 'cgt' or 'cfdt' or 'cfe-cgc' or 'fo' or 'force' or 'ouvrière' or 'unsa' not in prev_sentence:\n",
    "                # Use regular expressions to match numerical values in both sentences\n",
    "                match_prev = re.search(r'(\\d+(\\.\\d+)?)', prev_sentence[::-1])  # Reverse the sentence for matching from the end\n",
    "                match_next = re.search(r'(\\d+(\\.\\d+)?)', next_sentence) \n",
    "                \n",
    "                if match_prev:\n",
    "                    numerical_values.append(match_prev.group(0)[::-1])  # Reverse the matched value to restore original order\n",
    "                \n",
    "                if match_next:\n",
    "                    numerical_values.append(match_next.group(0))\n",
    "\n",
    "    return numerical_values\n",
    "\n",
    "\n",
    "def store_to_excel(numerical_values):\n",
    "    df = pd.DataFrame({'AG': numerical_values})\n",
    "    df.to_excel('output.xlsx', index=False)\n",
    "\n",
    "\n",
    "# Read the text from the file\n",
    "with open('test.txt', 'r') as file:\n",
    "    text = file.read()\n",
    "    \n",
    "keywords = [\"générale\", \"individuelles\"]\n",
    "\n",
    "numerical_values = extract_numerical_values(text, keywords)\n",
    "store_to_excel(numerical_values)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attempt 2.5.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import re\n",
    "\n",
    "def extract_numerical_values(text, keywords):\n",
    "    numerical_values = []\n",
    "\n",
    "    # Tokenize the text into sentences\n",
    "    sentences = re.split(r'(?<!\\d)\\.\\s*', text)\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('french'))\n",
    "\n",
    "    # Iterate over each sentence\n",
    "    for sentence in sentences:\n",
    "        # Tokenize the sentence into words\n",
    "        tokenizer = RegexpTokenizer(r'\\w+')\n",
    "        tokens = tokenizer.tokenize(sentence.lower())\n",
    "\n",
    "        # Filter tokens and check for numerical values near the keywords\n",
    "        filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "        for i in range(len(filtered_tokens)):\n",
    "            if filtered_tokens[i] in keywords:\n",
    "                # Retrieve the sentence before and after the keyword\n",
    "                prev_sentence = ' '.join(filtered_tokens[:i])\n",
    "                next_sentence = ' '.join(filtered_tokens[i+1:])\n",
    "\n",
    "                # Exclude cases where numerical value is near unwanted words\n",
    "                if all(word not in prev_sentence for word in ['cgt', 'cfdt', 'cfe-cgc', 'fo', 'force', 'ouvrière', 'unsa']):\n",
    "                    # Use regular expressions to match numerical values in both sentences\n",
    "                    match_prev = re.search(r'(\\d+(\\.\\d+)?)', prev_sentence[::-1])  # Reverse the sentence for matching from the end\n",
    "                    match_next = re.search(r'(\\d+(\\.\\d+)?)', next_sentence)\n",
    "\n",
    "                    if match_prev:\n",
    "                        numerical_values.append(match_prev.group(0)[::-1])  # Reverse the matched value to restore original order\n",
    "\n",
    "                    if match_next:\n",
    "                        numerical_values.append(match_next.group(0))\n",
    "\n",
    "    return numerical_values\n",
    "\n",
    "\n",
    "def store_to_excel(numerical_values):\n",
    "    df = pd.DataFrame({'AG': numerical_values})\n",
    "    df.to_excel('output.xlsx', index=False)\n",
    "\n",
    "\n",
    "# Read the text from the file\n",
    "with open('test.txt', 'r') as file:\n",
    "    text = file.read()\n",
    "\n",
    "keywords = [\"générale\", \"individuelles\"]\n",
    "\n",
    "numerical_values = extract_numerical_values(text, keywords)\n",
    "store_to_excel(numerical_values)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keyword by type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "All arrays must be of the same length",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 80\u001b[0m\n\u001b[0;32m     77\u001b[0m numerical_values_AG, numerical_values_AI \u001b[39m=\u001b[39m extract_numerical_values(cleaned_text, keywords)\n\u001b[0;32m     79\u001b[0m \u001b[39m# Store numerical values in Excel\u001b[39;00m\n\u001b[1;32m---> 80\u001b[0m store_to_excel(numerical_values_AG, numerical_values_AI)\n",
      "Cell \u001b[1;32mIn[4], line 57\u001b[0m, in \u001b[0;36mstore_to_excel\u001b[1;34m(numerical_values_AG, numerical_values_AI)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstore_to_excel\u001b[39m(numerical_values_AG, numerical_values_AI):\n\u001b[0;32m     56\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(numerical_values_AG) \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(numerical_values_AI):\n\u001b[1;32m---> 57\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mAll arrays must be of the same length\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     59\u001b[0m     df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame({\u001b[39m'\u001b[39m\u001b[39mAG\u001b[39m\u001b[39m'\u001b[39m: numerical_values_AG, \u001b[39m'\u001b[39m\u001b[39mAI\u001b[39m\u001b[39m'\u001b[39m: numerical_values_AI})\n\u001b[0;32m     60\u001b[0m     df\u001b[39m.\u001b[39mto_excel(\u001b[39m'\u001b[39m\u001b[39mcollect.xlsx\u001b[39m\u001b[39m'\u001b[39m, index\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[1;31mValueError\u001b[0m: All arrays must be of the same length"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    # Tokenize the text into sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "\n",
    "    # Remove stopwords and add line breaks between sentences\n",
    "    stop_words = set(stopwords.words('french'))\n",
    "    cleaned_sentences = []\n",
    "    for sentence in sentences:\n",
    "        tokens = word_tokenize(sentence.lower())\n",
    "        filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "        cleaned_sentence = ' '.join(filtered_tokens)\n",
    "        cleaned_sentences.append(cleaned_sentence)\n",
    "\n",
    "    cleaned_text = '\\n'.join(cleaned_sentences)\n",
    "    return cleaned_text\n",
    "\n",
    "def extract_numerical_values(text, keywords):\n",
    "    numerical_values_AG = []\n",
    "    numerical_values_AI = []\n",
    "\n",
    "    sentences = text.split('\\n')\n",
    "\n",
    "    for sentence in sentences:\n",
    "        tokens = word_tokenize(sentence)\n",
    "        for i in range(len(tokens)):\n",
    "            if tokens[i] in keywords['AG']:\n",
    "                prev_sentence = ' '.join(tokens[:i])\n",
    "                next_sentence = ' '.join(tokens[i+1:])\n",
    "                if all(word not in prev_sentence for word in ['cgt', 'cfdt', 'cfe-cgc', 'fo', 'force', 'ouvrière', 'unsa']):\n",
    "                    match_prev = re.search(r'(\\d+(\\.\\d+)?)', prev_sentence[::-1])\n",
    "                    match_next = re.search(r'(\\d+(\\.\\d+)?)', next_sentence)\n",
    "                    if match_prev:\n",
    "                        numerical_values_AG.append(match_prev.group(0)[::-1])\n",
    "                    if match_next:\n",
    "                        numerical_values_AG.append(match_next.group(0))\n",
    "            elif tokens[i] in keywords['AI']:\n",
    "                prev_sentence = ' '.join(tokens[:i])\n",
    "                next_sentence = ' '.join(tokens[i+1:])\n",
    "                if all(word not in prev_sentence for word in ['cgt', 'cfdt', 'cfe-cgc', 'fo', 'force', 'ouvrière', 'unsa']):\n",
    "                    match_prev = re.search(r'(\\d+(\\.\\d+)?)', prev_sentence[::-1])\n",
    "                    match_next = re.search(r'(\\d+(\\.\\d+)?)', next_sentence)\n",
    "                    if match_prev:\n",
    "                        numerical_values_AI.append(match_prev.group(0)[::-1])\n",
    "                    if match_next:\n",
    "                        numerical_values_AI.append(match_next.group(0))\n",
    "\n",
    "    return numerical_values_AG, numerical_values_AI\n",
    "\n",
    "def store_to_excel(numerical_values_AG, numerical_values_AI):\n",
    "    if len(numerical_values_AG) != len(numerical_values_AI):\n",
    "        raise ValueError(\"All arrays must be of the same length\")\n",
    "\n",
    "    df = pd.DataFrame({'AG': numerical_values_AG, 'AI': numerical_values_AI})\n",
    "    df.to_excel('collect.xlsx', index=False)\n",
    "\n",
    "\n",
    "# Read the text from the file\n",
    "with open('test.txt', 'r') as file:\n",
    "    text = file.read()\n",
    "\n",
    "# Define the keywords for AG and AI\n",
    "keywords = {\n",
    "    'AG': [\"générale\", \"générales\", \"base\"],\n",
    "    'AI': [\"individuelle\", \"individuelles\", \"mérite\"]\n",
    "}\n",
    "\n",
    "# Clean the text\n",
    "cleaned_text = clean_text(text)\n",
    "\n",
    "# Extract numerical values\n",
    "numerical_values_AG, numerical_values_AI = extract_numerical_values(cleaned_text, keywords)\n",
    "\n",
    "# Store numerical values in Excel\n",
    "store_to_excel(numerical_values_AG, numerical_values_AI)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    # Tokenize the text into sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "\n",
    "    # Remove stopwords and add line breaks between sentences\n",
    "    stop_words = set(stopwords.words('french'))\n",
    "    cleaned_sentences = []\n",
    "    for sentence in sentences:\n",
    "        tokens = word_tokenize(sentence.lower())\n",
    "        filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "        cleaned_sentence = ' '.join(filtered_tokens)\n",
    "        cleaned_sentences.append(cleaned_sentence)\n",
    "\n",
    "    cleaned_text = '\\n'.join(cleaned_sentences)\n",
    "    return cleaned_text\n",
    "\n",
    "def extract_numerical_values(text, keywords):\n",
    "    numerical_values_AG = []\n",
    "    numerical_values_AI = []\n",
    "\n",
    "    sentences = text.split('\\n')\n",
    "\n",
    "    for sentence in sentences:\n",
    "        tokens = word_tokenize(sentence)\n",
    "        for i in range(len(tokens)):\n",
    "            if tokens[i] in keywords['AG']:\n",
    "                prev_sentence = ' '.join(tokens[:i])\n",
    "                next_sentence = ' '.join(tokens[i+1:])\n",
    "                if all(word not in prev_sentence for word in ['cgt', 'cfdt', 'cfe-cgc', 'fo', 'force', 'ouvrière', 'unsa']):\n",
    "                    match_prev = re.search(r'(\\d+(\\.\\d+)?)', prev_sentence[::-1])\n",
    "                    match_next = re.search(r'(\\d+(\\.\\d+)?)', next_sentence)\n",
    "                    if match_prev:\n",
    "                        numerical_values_AG.append(match_prev.group(0)[::-1])\n",
    "                    if match_next:\n",
    "                        numerical_values_AG.append(match_next.group(0))\n",
    "            elif tokens[i] in keywords['AI']:\n",
    "                prev_sentence = ' '.join(tokens[:i])\n",
    "                next_sentence = ' '.join(tokens[i+1:])\n",
    "                if all(word not in prev_sentence for word in ['cgt', 'cfdt', 'cfe-cgc', 'fo', 'force', 'ouvrière', 'unsa']):\n",
    "                    match_prev = re.search(r'(\\d+(\\.\\d+)?)', prev_sentence[::-1])\n",
    "                    match_next = re.search(r'(\\d+(\\.\\d+)?)', next_sentence)\n",
    "                    if match_prev:\n",
    "                        numerical_values_AI.append(match_prev.group(0)[::-1])\n",
    "                    if match_next:\n",
    "                        numerical_values_AI.append(match_next.group(0))\n",
    "\n",
    "    return numerical_values_AG, numerical_values_AI\n",
    "\n",
    "def store_to_excel(numerical_values_AG, numerical_values_AI):\n",
    "    if len(numerical_values_AG) != len(numerical_values_AI):\n",
    "        raise ValueError(\"All arrays must be of the same length\")\n",
    "\n",
    "    df = pd.DataFrame({'AG': numerical_values_AG, 'AI': numerical_values_AI})\n",
    "    df.to_excel('collect.xlsx', index=False)\n",
    "\n",
    "\n",
    "# Read the text from the file\n",
    "with open('test.txt', 'r') as file:\n",
    "    text = file.read()\n",
    "\n",
    "# Define the keywords for AG and AI\n",
    "keywords = {\n",
    "    'AG': [\"générale\", \"générales\", \"base\"],\n",
    "    'AI': [\"individuelle\", \"individuelles\", \"mérite\"]\n",
    "}\n",
    "\n",
    "# Clean the text\n",
    "cleaned_text = clean_text(text)\n",
    "\n",
    "# Extract numerical values\n",
    "numerical_values_AG, numerical_values_AI = extract_numerical_values(cleaned_text, keywords)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New clean function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accord d’entreprise relatif Négociation Annuelle Obligatoire rémunération, temps travail partage valeur ajoutée l’année 2023 Entre Société : CHUGAI PHARMA FRANCE SAS, société anonyme simplifiée, immatriculée Registre Commerce Sociétés Nanterre sous numéro B 435 074 422, capital 1.000.000 d’euros dont siège social situé Tour Pacific, 11 Cour Valmy, 92800 PUTEAUX Représentée xxx agissant qualité Président D’une part, L’organisation syndicale UNSA représentative Société, représentée xxx, délégué syndical, dûment habilité, D’autre part, Préambule 15 novembre 2022, parties convenues calendrier modalités tenue réunions ainsi informations remettre l’organisation syndicale représentative Société\n",
      "cours réunions 25 novembre 2022, 15 décembre 2022 10 janvier 2022, parties échangé thèmes prévus articles\n",
      "2242-1 1° suivants Code travail, savoir rémunération, temps travail partage valeur ajoutée l'entreprise\n",
      "ici rappelé thèmes « Temps travail », « Partage valeur ajoutée » « Égalité professionnelle » font l’objet d’accords collectifs d’entreprise spécifiques, vigueur\n",
      "L’organisation syndicale a communiqué propositions, auxquelles Direction a répondu\n",
      "Etat dernières propositions l’UNSA : - Rémunérations : Augmentation générale 6 % - Prime partage valeur (PPV): Demande d’une prime 3000 € salariés figurant l’effectif société - Épargne salariale : demande d'abondement PERCO, hauteur 10 % - RP, WE jours travaillés : demande revalorisation 10 € RP soirs samedis travaillés 20 € dimanches jours fériés travaillés A l’issue négociation, a convenu arrêté suit : Article 1 - Champ d’application\n",
      "présent accord concerne l’ensemble salariés Société\n",
      "Article 2 - Durée\n",
      "présent accord conclu durée indéterminée compter signature\n",
      "Article 3 – Dispositions relatives Rémunération 3.1 - Égalité professionnelle entre hommes femmes\n",
      "cours négociations, parties discuté l’application principe d’égalité professionnelle entre femmes hommes sein l’entreprise\n",
      "a rappelé mesures spécifiques relatives l’égalité rémunération entre hommes femmes convenues termes l’accord collectif d’entreprise relatif l’Egalité professionnelle H/F conclu 1er mai 2022 durée 4 ans\n",
      "a notamment constaté taux l'Index Egalité Professionnelle s'élève 89/100 l’année 2021. parties conclu qu’il n’y mesures supplémentaires prévoir cadre présente NAO\n",
      "3.2 - Augmentations annuelles salaires taux moyen augmentations annuelles individuelles effectif 1er janvier 2023 décidé Direction, appliqué salaires base, 4.5 % hors promotion\n",
      "taux comprend titre exceptionnel, augmentation minimum garanti 2% appliquée salaires base l’ensemble salariés, effet 1er janvier 2023. 3.3 – Indemnisation RP, week-end jours fériés RP collaborateurs concernés revalorisés compter 1er janvier 2023. montants applicables suivants : - RP soir : 130 € bruts (au lieu 120 € bruts) - Samedi travaillé : 170 € bruts (au lieu 160 € bruts) - Dimanche jour férié travaillé : 200 € bruts (au lieu 180 € bruts) Article 4 – Budget Activités Sociales Culturelles Comité Social Économique rappel, revalorisation budget Activités Sociales Culturelles Comité Social Économique appliquée conformément l’accord mise place CSE signé 15 septembre 2022. taux porté 0.74 % (au lieu 0.68%) masse salariale annuelle brute telle définie l’article L2312-83 Code Travail compter 1er janvier 2023. Article 5 – Prime Partage Valeur rappel, décision unilatérale Direction date 21 décembre 2022, prime partage valeur versée janvier 2023. montant prime modulé selon conditions critères cumulatifs définis ladite décision\n",
      "Article 6 – Révision Dénonciation 6.1 Révision Toute demande révision devra être adressée lettre recommandée accusé réception chacune autres Parties signataires adhérentes comporter, outre l’indication dispositions dont révision demandée, propositions remplacement\n",
      "plus rapidement possible plus tard délai trois (3) mois suivant réception cette lettre, Parties sus indiquées devront ouvrir négociation vue rédaction éventuelle d’un nouveau texte\n",
      "dispositions l’accord dont révision demandée resteront vigueur jusqu’à conclusion d’un nouvel accord\n",
      "dispositions l’avenant portant révision, substitueront plein droit celles l’accord qu’elles modifient opposables l’entreprise salariés liés l’accord, date expressément convenue, défaut partir jour suivra dépôt\n",
      "6.2 Dénonciation présent accord pourra être dénoncé l'une l'autre parties signataires respectant préavis 3 mois\n",
      "partie dénonce l'accord doit notifier cette décision, lettre recommandée accusé réception, DRIEETS lettre recommandée accusé réception autres parties signataires\n",
      "Article 8 – Publicité dépôt présent accord notifié, remise main propre contre décharge auprès Délégué syndical, l'organisation syndicale représentative Société\n",
      "porté connaissance personnel affichage version électronique\n",
      "présent accord déposé deux exemplaires (une version support électronique format PDF version anonymisée support électronique format\n",
      "docx, plateforme téléprocédure ministère travail www\n",
      "teleaccords\n",
      "travail-emploi\n",
      "gouv\n",
      "fr) exemplaire (version papier signée parties) greffe Conseil prud’hommes Nanterre\n",
      "Fait Paris Défense, 11 janvier 2023, Direction : xxx Président l’organisation syndicale : L’organisation syndicale UNSA, représentée xxx, Délégué Syndical\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def clean(text):\n",
    "    # Tokenize the text into sentences\n",
    "    sentences = re.split(r'(?<!\\d)\\.\\s*', text)\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('french'))\n",
    "\n",
    "    # Tokenize each sentence and remove stopwords\n",
    "    cleaned_sentences = []\n",
    "    for sentence in sentences:\n",
    "        tokens = sentence.split()\n",
    "        cleaned_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "        cleaned_sentence = ' '.join(cleaned_tokens)\n",
    "        cleaned_sentences.append(cleaned_sentence)\n",
    "\n",
    "    # Join cleaned sentences with a line break\n",
    "    cleaned_text = '\\n'.join(cleaned_sentences)\n",
    "\n",
    "    return cleaned_text\n",
    "\n",
    "with open('test.txt', 'r') as file:\n",
    "    text = file.read()\n",
    "\n",
    "cleaned_text = clean(text)\n",
    "print(cleaned_text)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collecting full paragraph "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective here is to make reading more productive. The code has for objective to go through each word document and only extract the relevant paragraphs. It can takes two forms: \n",
    "1. Modify individually each document and only keep the relevant paragraphs\n",
    "2. Directly extract meta information and paparagrahs and write it on a third document "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In-document modification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Highlighting algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx import Document\n",
    "from docx.enum.text import WD_COLOR\n",
    "import pandas as pd\n",
    "import glob\n",
    "import re\n",
    "\n",
    "df = pd.read_excel('keywords.xlsx')\n",
    "list_of_words = df['keywords'].tolist()\n",
    "\n",
    "# Setup regex\n",
    "patterns = [r'\\b' + word + r'\\b' for word in list_of_words]\n",
    "re_highlight = re.compile('(' + '|'.join(p for p in patterns) + ')+', re.IGNORECASE)\n",
    "\n",
    "folder_path = '.\\\\sample.docx' \n",
    "\n",
    "docx_files = glob.glob(folder_path + '\\\\*.docx')\n",
    "\n",
    "for file in docx_files:\n",
    "    doc = Document(file)\n",
    "    for para in doc.paragraphs:\n",
    "        text = para.text\n",
    "        if len(re_highlight.findall(text)) > 0:\n",
    "            matches = re_highlight.finditer(text)\n",
    "            para.text = ''\n",
    "            p3 = 0\n",
    "            for match in matches:\n",
    "                p1 = p3\n",
    "                p2, p3 = match.span()\n",
    "                para.add_run(text[p1:p2])\n",
    "                run = para.add_run(text[p2:p3])\n",
    "                run.font.highlight_color = WD_COLOR.YELLOW\n",
    "                para.add_run(text[p3:])\n",
    "    doc.save(file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keeping only highlighted paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx import Document\n",
    "from docx.enum.text import WD_COLOR\n",
    "import pandas as pd\n",
    "import glob\n",
    "import re\n",
    "\n",
    "# Import keywords from the Excel file and turn them into a list\n",
    "df = pd.read_excel('keywords.xlsx')\n",
    "list_of_words = df['keywords'].tolist()\n",
    "\n",
    "# Set up regex\n",
    "patterns = [r'\\b' + word + r'\\b' for word in list_of_words]\n",
    "re_highlight = re.compile('(' + '|'.join(p for p in patterns) + ')+', re.IGNORECASE)\n",
    "\n",
    "def process_docx_files(folder_path):\n",
    "    docx_files = glob.glob(folder_path + '\\\\*.docx')\n",
    "    for file in docx_files:\n",
    "        doc = Document(file)\n",
    "        modified_doc = Document()  # Create a new document to store modified paragraphs\n",
    "        for para in doc.paragraphs:\n",
    "            text = para.text\n",
    "            if len(re_highlight.findall(text)) > 0:\n",
    "                matches = re_highlight.finditer(text)\n",
    "                p3 = 0\n",
    "                highlighted_para = modified_doc.add_paragraph()  # Add a new paragraph to the modified document\n",
    "                for match in matches:\n",
    "                    p1 = p3\n",
    "                    p2, p3 = match.span()\n",
    "                    highlighted_para.add_run(text[p1:p2])\n",
    "                    run = highlighted_para.add_run(text[p2:p3])\n",
    "                    run.font.highlight_color = WD_COLOR.YELLOW\n",
    "                    highlighted_para.add_run(text[p3:])\n",
    "        if modified_doc.paragraphs:  # Only save the modified document if it contains highlighted paragraphs\n",
    "            modified_doc.save(file)\n",
    "\n",
    "# Provide the folder path where the algorithm will iterate over all *.docx files\n",
    "folder_path = '.\\\\sample.docx'\n",
    "process_docx_files(folder_path)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paragraph extraction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code has for objective to take all the extracted paragraphs and meta data and store them in a single file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx import Document\n",
    "from docx.enum.text import WD_COLOR\n",
    "import pandas as pd\n",
    "import glob\n",
    "import re\n",
    "\n",
    "# Import keywords from the Excel file and turn them into a list\n",
    "df = pd.read_excel('keywords.xlsx')\n",
    "list_of_words = df['keywords'].tolist()\n",
    "\n",
    "# Set up regex\n",
    "patterns = [r'\\b' + word + r'\\b' for word in list_of_words]\n",
    "re_highlight = re.compile('(' + '|'.join(p for p in patterns) + ')+', re.IGNORECASE)\n",
    "\n",
    "def process_docx_files(folder_path):\n",
    "    docx_files = glob.glob(folder_path + '\\\\*.docx')\n",
    "    for file in docx_files:\n",
    "        doc = Document(file)\n",
    "        modified_doc = Document()  # Create a new document to store modified paragraphs\n",
    "        file_name = file.split('.')[0]  # Extract the name of the file\n",
    "        modified_doc.add_paragraph(file_name, style='Heading 1')  # Add the file name to the modified document with bold formatting\n",
    "        for para in doc.paragraphs:\n",
    "            text = para.text\n",
    "            if len(re_highlight.findall(text)) > 0:\n",
    "                matches = re_highlight.finditer(text)\n",
    "                p3 = 0\n",
    "                highlighted_para = modified_doc.add_paragraph()  # Add a new paragraph to the modified document\n",
    "                for match in matches:\n",
    "                    p1 = p3\n",
    "                    p2, p3 = match.span()\n",
    "                    highlighted_para.add_run(text[p1:p2])\n",
    "                    run = highlighted_para.add_run(text[p2:p3])\n",
    "                    run.font.highlight_color = WD_COLOR.YELLOW\n",
    "                    highlighted_para.add_run(text[p3:])\n",
    "        if modified_doc.paragraphs:  # Only save the modified document if it contains highlighted paragraphs\n",
    "            modified_doc.save(file)\n",
    "\n",
    "# Provide the folder path where the algorithm will iterate over all *.docx files\n",
    "folder_path = '.\\\\sample.docx'\n",
    "process_docx_files(folder_path)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It Works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"no style with name 'Heading 1'\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[60], line 49\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[39m# Provide the folder path where the algorithm will iterate over all *.docx files\u001b[39;00m\n\u001b[0;32m     48\u001b[0m folder_path \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39msample.docx\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m---> 49\u001b[0m process_docx_files(folder_path)\n",
      "Cell \u001b[1;32mIn[60], line 27\u001b[0m, in \u001b[0;36mprocess_docx_files\u001b[1;34m(folder_path)\u001b[0m\n\u001b[0;32m     25\u001b[0m doc \u001b[39m=\u001b[39m Document(file)\n\u001b[0;32m     26\u001b[0m title \u001b[39m=\u001b[39m extract_file_name(file)  \u001b[39m# Extract the file name without extension\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m modified_doc \u001b[39m=\u001b[39m doc\u001b[39m.\u001b[39;49madd_heading(title, \u001b[39m1\u001b[39;49m)\n\u001b[0;32m     28\u001b[0m modified_doc \u001b[39m=\u001b[39m Document()  \u001b[39m# Create a new document to store modified paragraphs\u001b[39;00m\n\u001b[0;32m     29\u001b[0m title \u001b[39m=\u001b[39m doc\u001b[39m.\u001b[39mcore_properties\u001b[39m.\u001b[39mtitle  \u001b[39m# Extract the title of the document\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\docx\\document.py:39\u001b[0m, in \u001b[0;36mDocument.add_heading\u001b[1;34m(self, text, level)\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mlevel must be in range 0-9, got \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m level)\n\u001b[0;32m     38\u001b[0m style \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mTitle\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m level \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mHeading \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m level\n\u001b[1;32m---> 39\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madd_paragraph(text, style)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\docx\\document.py:56\u001b[0m, in \u001b[0;36mDocument.add_paragraph\u001b[1;34m(self, text, style)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39madd_paragraph\u001b[39m(\u001b[39mself\u001b[39m, text\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m, style\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m     48\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[39m    Return a paragraph newly added to the end of the document, populated\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[39m    with *text* and having paragraph style *style*. *text* can contain\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[39m    break.\u001b[39;00m\n\u001b[0;32m     55\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 56\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_body\u001b[39m.\u001b[39;49madd_paragraph(text, style)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\docx\\blkcntnr.py:39\u001b[0m, in \u001b[0;36mBlockItemContainer.add_paragraph\u001b[1;34m(self, text, style)\u001b[0m\n\u001b[0;32m     37\u001b[0m     paragraph\u001b[39m.\u001b[39madd_run(text)\n\u001b[0;32m     38\u001b[0m \u001b[39mif\u001b[39;00m style \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m---> 39\u001b[0m     paragraph\u001b[39m.\u001b[39;49mstyle \u001b[39m=\u001b[39m style\n\u001b[0;32m     40\u001b[0m \u001b[39mreturn\u001b[39;00m paragraph\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\docx\\text\\paragraph.py:110\u001b[0m, in \u001b[0;36mParagraph.style\u001b[1;34m(self, style_or_name)\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[39m@style\u001b[39m\u001b[39m.\u001b[39msetter\n\u001b[0;32m    109\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstyle\u001b[39m(\u001b[39mself\u001b[39m, style_or_name):\n\u001b[1;32m--> 110\u001b[0m     style_id \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpart\u001b[39m.\u001b[39;49mget_style_id(\n\u001b[0;32m    111\u001b[0m         style_or_name, WD_STYLE_TYPE\u001b[39m.\u001b[39;49mPARAGRAPH\n\u001b[0;32m    112\u001b[0m     )\n\u001b[0;32m    113\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_p\u001b[39m.\u001b[39mstyle \u001b[39m=\u001b[39m style_id\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\docx\\parts\\document.py:78\u001b[0m, in \u001b[0;36mDocumentPart.get_style_id\u001b[1;34m(self, style_or_name, style_type)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_style_id\u001b[39m(\u001b[39mself\u001b[39m, style_or_name, style_type):\n\u001b[0;32m     71\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     72\u001b[0m \u001b[39m    Return the style_id (|str|) of the style of *style_type* matching\u001b[39;00m\n\u001b[0;32m     73\u001b[0m \u001b[39m    *style_or_name*. Returns |None| if the style resolves to the default\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[39m    present in the document.\u001b[39;00m\n\u001b[0;32m     77\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 78\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstyles\u001b[39m.\u001b[39;49mget_style_id(style_or_name, style_type)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\docx\\styles\\styles.py:109\u001b[0m, in \u001b[0;36mStyles.get_style_id\u001b[1;34m(self, style_or_name, style_type)\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_style_id_from_style(style_or_name, style_type)\n\u001b[0;32m    108\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 109\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_style_id_from_name(style_or_name, style_type)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\docx\\styles\\styles.py:139\u001b[0m, in \u001b[0;36mStyles._get_style_id_from_name\u001b[1;34m(self, style_name, style_type)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_style_id_from_name\u001b[39m(\u001b[39mself\u001b[39m, style_name, style_type):\n\u001b[0;32m    133\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    134\u001b[0m \u001b[39m    Return the id of the style of *style_type* corresponding to\u001b[39;00m\n\u001b[0;32m    135\u001b[0m \u001b[39m    *style_name*. Returns |None| if that style is the default style for\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[39m    *style_type*. Raises |ValueError| if the named style is not found in\u001b[39;00m\n\u001b[0;32m    137\u001b[0m \u001b[39m    the document or does not match *style_type*.\u001b[39;00m\n\u001b[0;32m    138\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 139\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_style_id_from_style(\u001b[39mself\u001b[39;49m[style_name], style_type)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\docx\\styles\\styles.py:53\u001b[0m, in \u001b[0;36mStyles.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m     50\u001b[0m     warn(msg, \u001b[39mUserWarning\u001b[39;00m, stacklevel\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[0;32m     51\u001b[0m     \u001b[39mreturn\u001b[39;00m StyleFactory(style_elm)\n\u001b[1;32m---> 53\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mno style with name \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m key)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"no style with name 'Heading 1'\""
     ]
    }
   ],
   "source": [
    "from docx import Document\n",
    "from docx.enum.text import WD_COLOR\n",
    "import pandas as pd\n",
    "import glob\n",
    "import re\n",
    "\n",
    "# Import keywords from the Excel file and turn them into a list\n",
    "df = pd.read_excel('keywords.xlsx')\n",
    "list_of_words = df['keywords'].tolist()\n",
    "\n",
    "# Set up regex\n",
    "patterns = [r'\\b' + word + r'\\b' for word in list_of_words]\n",
    "re_highlight = re.compile('(' + '|'.join(p for p in patterns) + ')+', re.IGNORECASE)\n",
    "\n",
    "# Define a function to extract the file name without extension\n",
    "def extract_file_name(file):\n",
    "    file_name = os.path.basename(file)\n",
    "    file_name_without_extension = os.path.splitext(file_name)[0]\n",
    "    return file_name_without_extension\n",
    "\n",
    "# Define a function to process all *.docx files in a folder\n",
    "def process_docx_files(folder_path):\n",
    "    docx_files = glob.glob(folder_path + '\\\\*.docx')\n",
    "    for file in docx_files:\n",
    "        doc = Document(file)\n",
    "        title = extract_file_name(file)  # Extract the file name without extension\n",
    "        modified_doc = doc.add_heading(title, level = 1)\n",
    "        modified_doc = Document()  # Create a new document to store modified paragraphs\n",
    "        title = doc.core_properties.title  # Extract the title of the document\n",
    "        modified_doc.add_heading(title)  # Add the title as Header 1 to the modified document\n",
    "        for para in doc.paragraphs:\n",
    "            text = para.text\n",
    "            if len(re_highlight.findall(text)) > 0:\n",
    "                matches = re_highlight.finditer(text)\n",
    "                p3 = 0\n",
    "                highlighted_para = modified_doc.add_paragraph()  # Add a new paragraph to the modified document\n",
    "                for match in matches:\n",
    "                    p1 = p3\n",
    "                    p2, p3 = match.span()\n",
    "                    highlighted_para.add_run(text[p1:p2])\n",
    "                    run = highlighted_para.add_run(text[p2:p3])\n",
    "                    run.font.highlight_color = WD_COLOR.YELLOW\n",
    "                    highlighted_para.add_run(text[p3:])\n",
    "        if modified_doc.paragraphs:  # Only save the modified document if it contains highlighted paragraphs\n",
    "            modified_doc.save(file)\n",
    "\n",
    "# Provide the folder path where the algorithm will iterate over all *.docx files\n",
    "folder_path = '.\\\\sample.docx'\n",
    "process_docx_files(folder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           File Name\n",
      "0   T00123005469-44374815700049.docx\n",
      "1   T00323002451-50325890700032.docx\n",
      "2   T00622007346-30737848900032.docx\n",
      "3   T00623007972-41735031100011.docx\n",
      "4   T00623008174-41120016500016.docx\n",
      "5   T01322016435-05781979900024.docx\n",
      "6   T01422006482-50270701100024.docx\n",
      "7   T01423006943-50270701100024.docx\n",
      "8   T01423006961-82032462200027.docx\n",
      "9   T02122005184-80975930100025.docx\n",
      "10  T02123005896-50918616900025.docx\n",
      "11  T02722003222-44025100700025.docx\n",
      "12  T02722003507-90846191600029.docx\n",
      "13  T02822002721-54295011800019.docx\n",
      "14  T02822002949-45137563800017.docx\n",
      "15  T02823003173-72200005600035.docx\n",
      "16  T03322012084-49038709900024.docx\n",
      "17  T03323012438-39256238500014.docx\n",
      "18  T03523013069-32829784100012.docx\n",
      "19  T03722003682-44780501100015.docx\n",
      "20  T03823012635-40960061600026.docx\n",
      "21  T04923009278-35329865600029.docx\n",
      "22  T06122002285-87802758000025.docx\n",
      "23  T06222007558-57578017600039.docx\n",
      "24  T06322005226-43255499600020.docx\n",
      "25  T06722010202-31625229500027.docx\n",
      "26  T06723011929-48107570300043.docx\n",
      "27  T06922019386-44780021000018.docx\n",
      "28  T06922021827-41866103900044.docx\n",
      "29  T06922023874-39929538500070.docx\n",
      "30  T07023001624-67625011100017.docx\n",
      "31  T07223004886-43401899000011.docx\n",
      "32  T07422005681-53950120500022.docx\n",
      "33  T07423006663-44013977200048.docx\n",
      "34  T07622007771-79361882800024.docx\n",
      "35  T08422003665-54295011800084.docx\n",
      "36  T08423004312-54295011800084.docx\n",
      "37  T08622002517-84492311000028.docx\n",
      "38  T08623002925-35318984800016.docx\n",
      "39  T08623002926-84492311000028.docx\n",
      "40  T08923002224-37850692700078.docx\n",
      "41  T09122008688-39771069000017.docx\n",
      "42  T09122008958-31226189400033.docx\n",
      "43  T09123009722-49137116700017.docx\n",
      "44  T09222037802-61203061900102.docx\n",
      "45  T09223039581-43507442200039.docx\n",
      "46  T09223039671-80324795600011.docx\n",
      "47  T09223039783-55820107500071.docx\n",
      "48  T09223040217-30515140900136.docx\n",
      "49  T09223040755-57222339400048.docx\n",
      "50  T09223041014-80854037100016.docx\n",
      "51  T59L23019681-44813963400019.docx\n",
      "52                          Tutoriel\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "def extract_file_name(file):\n",
    "    file_name = os.path.basename(file)\n",
    "    file_name_without_extension = os.path.splitext(file_name)[0]\n",
    "    return file_name_without_extension\n",
    "\n",
    "folder_path = '.\\\\sample.docx'\n",
    "docx_files = glob.glob(os.path.join(folder_path, '*.docx'))\n",
    "\n",
    "file_names = [extract_file_name(file) for file in docx_files]\n",
    "df = pd.DataFrame({'File Name': file_names})\n",
    "print(df)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Final version with Title**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx import Document\n",
    "from docx.enum.text import WD_COLOR\n",
    "import pandas as pd\n",
    "import glob\n",
    "import re\n",
    "import os\n",
    "\n",
    "# Import keywords from the Excel file and turn them into a list\n",
    "df = pd.read_excel('keywords.xlsx')\n",
    "list_of_words = df['keywords'].tolist()\n",
    "\n",
    "# Set up regex\n",
    "patterns = [r'\\b' + word + r'\\b' for word in list_of_words]\n",
    "re_highlight = re.compile('(' + '|'.join(p for p in patterns) + ')+', re.IGNORECASE)\n",
    "\n",
    "# Define a function to extract the file name without extension\n",
    "def extract_file_name(file):\n",
    "    file_name = os.path.basename(file)\n",
    "    file_name_without_extension = os.path.splitext(file_name)[0]\n",
    "    return file_name_without_extension\n",
    "\n",
    "# Define a function to process all *.docx files in a folder\n",
    "def process_docx_files(folder_path):\n",
    "    docx_files = glob.glob(folder_path + '\\\\*.docx')\n",
    "    for file in docx_files:\n",
    "        doc = Document(file)\n",
    "        title = extract_file_name(file)  # Extract the file name without extension\n",
    "        modified_doc = Document()  # Create a new document to store modified paragraphs\n",
    "        modified_doc.add_heading(title, level=1)  # Add the title as Header 1 to the modified document\n",
    "        for para in doc.paragraphs:\n",
    "            text = para.text\n",
    "            if len(re_highlight.findall(text)) > 0:\n",
    "                matches = re_highlight.finditer(text)\n",
    "                p3 = 0\n",
    "                highlighted_para = modified_doc.add_paragraph()  # Add a new paragraph to the modified document\n",
    "                for match in matches:\n",
    "                    p1 = p3\n",
    "                    p2, p3 = match.span()\n",
    "                    highlighted_para.add_run(text[p1:p2])\n",
    "                    run = highlighted_para.add_run(text[p2:p3])\n",
    "                    run.font.highlight_color = WD_COLOR.YELLOW\n",
    "                    highlighted_para.add_run(text[p3:])\n",
    "        if modified_doc.paragraphs:  # Only save the modified document if it contains highlighted paragraphs\n",
    "            modified_doc.save(file)\n",
    "\n",
    "# Provide the folder path where the algorithm will iterate over all *.docx files\n",
    "folder_path = '.\\\\sample.docx'\n",
    "process_docx_files(folder_path)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'>' not supported between instances of '_Columns' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 42\u001b[0m\n\u001b[0;32m     39\u001b[0m output_doc_file \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39moutput.docx\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     41\u001b[0m \u001b[39m# Open Word document and extract table\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m open_word_document_and_extract_table(input_docx_file, output_doc_file)\n",
      "Cell \u001b[1;32mIn[6], line 29\u001b[0m, in \u001b[0;36mopen_word_document_and_extract_table\u001b[1;34m(docx_file, output_file)\u001b[0m\n\u001b[0;32m     26\u001b[0m document \u001b[39m=\u001b[39m docx\u001b[39m.\u001b[39mDocument(docx_file)\n\u001b[0;32m     28\u001b[0m \u001b[39m# Extract table and save to doc file\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m extract_table_from_docx(docx_file, output_file)\n\u001b[0;32m     31\u001b[0m \u001b[39m# Close Word document\u001b[39;00m\n\u001b[0;32m     32\u001b[0m document\u001b[39m.\u001b[39mclose()\n",
      "Cell \u001b[1;32mIn[6], line 21\u001b[0m, in \u001b[0;36mextract_table_from_docx\u001b[1;34m(docx_file, output_file)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mextract_table_from_docx\u001b[39m(docx_file, output_file):\n\u001b[0;32m     20\u001b[0m     table \u001b[39m=\u001b[39m retrieve_table_from_docx(docx_file)\n\u001b[1;32m---> 21\u001b[0m     create_doc_file(table, output_file)\n",
      "Cell \u001b[1;32mIn[6], line 12\u001b[0m, in \u001b[0;36mcreate_doc_file\u001b[1;34m(table, output_file)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate_doc_file\u001b[39m(table, output_file):\n\u001b[0;32m     11\u001b[0m     document \u001b[39m=\u001b[39m docx\u001b[39m.\u001b[39mDocument()\n\u001b[1;32m---> 12\u001b[0m     new_table \u001b[39m=\u001b[39m document\u001b[39m.\u001b[39;49madd_table(rows\u001b[39m=\u001b[39;49mtable\u001b[39m.\u001b[39;49mrows, cols\u001b[39m=\u001b[39;49mtable\u001b[39m.\u001b[39;49mcolumns)\n\u001b[0;32m     13\u001b[0m     \u001b[39mfor\u001b[39;00m i, row \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(table\u001b[39m.\u001b[39mrows):\n\u001b[0;32m     14\u001b[0m         \u001b[39mfor\u001b[39;00m j, cell \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(row\u001b[39m.\u001b[39mcells):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\docx\\document.py:92\u001b[0m, in \u001b[0;36mDocument.add_table\u001b[1;34m(self, rows, cols, style)\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39madd_table\u001b[39m(\u001b[39mself\u001b[39m, rows, cols, style\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m     86\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     87\u001b[0m \u001b[39m    Add a table having row and column counts of *rows* and *cols*\u001b[39;00m\n\u001b[0;32m     88\u001b[0m \u001b[39m    respectively and table style of *style*. *style* may be a paragraph\u001b[39;00m\n\u001b[0;32m     89\u001b[0m \u001b[39m    style object or a paragraph style name. If *style* is |None|, the\u001b[39;00m\n\u001b[0;32m     90\u001b[0m \u001b[39m    table inherits the default table style of the document.\u001b[39;00m\n\u001b[0;32m     91\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 92\u001b[0m     table \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_body\u001b[39m.\u001b[39;49madd_table(rows, cols, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_block_width)\n\u001b[0;32m     93\u001b[0m     table\u001b[39m.\u001b[39mstyle \u001b[39m=\u001b[39m style\n\u001b[0;32m     94\u001b[0m     \u001b[39mreturn\u001b[39;00m table\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\docx\\blkcntnr.py:49\u001b[0m, in \u001b[0;36mBlockItemContainer.add_table\u001b[1;34m(self, rows, cols, width)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[39mReturn a table of *width* having *rows* rows and *cols* columns,\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[39mnewly appended to the content in this container. *width* is evenly\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \u001b[39mdistributed between the table columns.\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mtable\u001b[39;00m \u001b[39mimport\u001b[39;00m Table\n\u001b[1;32m---> 49\u001b[0m tbl \u001b[39m=\u001b[39m CT_Tbl\u001b[39m.\u001b[39;49mnew_tbl(rows, cols, width)\n\u001b[0;32m     50\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_element\u001b[39m.\u001b[39m_insert_tbl(tbl)\n\u001b[0;32m     51\u001b[0m \u001b[39mreturn\u001b[39;00m Table(tbl, \u001b[39mself\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\docx\\oxml\\table.py:158\u001b[0m, in \u001b[0;36mCT_Tbl.new_tbl\u001b[1;34m(cls, rows, cols, width)\u001b[0m\n\u001b[0;32m    152\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[0;32m    153\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mnew_tbl\u001b[39m(\u001b[39mcls\u001b[39m, rows, cols, width):\n\u001b[0;32m    154\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    155\u001b[0m \u001b[39m    Return a new `w:tbl` element having *rows* rows and *cols* columns\u001b[39;00m\n\u001b[0;32m    156\u001b[0m \u001b[39m    with *width* distributed evenly between the columns.\u001b[39;00m\n\u001b[0;32m    157\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 158\u001b[0m     \u001b[39mreturn\u001b[39;00m parse_xml(\u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_tbl_xml(rows, cols, width))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\docx\\oxml\\table.py:185\u001b[0m, in \u001b[0;36mCT_Tbl._tbl_xml\u001b[1;34m(cls, rows, cols, width)\u001b[0m\n\u001b[0;32m    183\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[0;32m    184\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_tbl_xml\u001b[39m(\u001b[39mcls\u001b[39m, rows, cols, width):\n\u001b[1;32m--> 185\u001b[0m     col_width \u001b[39m=\u001b[39m Emu(width\u001b[39m/\u001b[39mcols) \u001b[39mif\u001b[39;00m cols \u001b[39m>\u001b[39;49m \u001b[39m0\u001b[39;49m \u001b[39melse\u001b[39;00m Emu(\u001b[39m0\u001b[39m)\n\u001b[0;32m    186\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m    187\u001b[0m         \u001b[39m'\u001b[39m\u001b[39m<w:tbl \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m>\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[0;32m    188\u001b[0m         \u001b[39m'\u001b[39m\u001b[39m  <w:tblPr>\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    200\u001b[0m         \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_trs_xml(rows, cols, col_width)\n\u001b[0;32m    201\u001b[0m     )\n",
      "\u001b[1;31mTypeError\u001b[0m: '>' not supported between instances of '_Columns' and 'int'"
     ]
    }
   ],
   "source": [
    "import docx\n",
    "\n",
    "# Function to retrieve a table from a docx document\n",
    "def retrieve_table_from_docx(docx_file):\n",
    "    document = docx.Document(docx_file)\n",
    "    table = document.tables[0]  # Assuming the table is the first table in the document\n",
    "    return table\n",
    "\n",
    "# Create a doc file to store the extracted table\n",
    "def create_doc_file(table, output_file):\n",
    "    document = docx.Document()\n",
    "    new_table = document.add_table(rows=table.rows, cols=table.columns)\n",
    "    for i, row in enumerate(table.rows):\n",
    "        for j, cell in enumerate(row.cells):\n",
    "            new_table.cell(i, j).text = cell.text\n",
    "    document.save(output_file)\n",
    "\n",
    "# Extract table from docx and save to doc file\n",
    "def extract_table_from_docx(docx_file, output_file):\n",
    "    table = retrieve_table_from_docx(docx_file)\n",
    "    create_doc_file(table, output_file)\n",
    "\n",
    "# Open a Word document and use the function\n",
    "def open_word_document_and_extract_table(docx_file, output_file):\n",
    "    # Open Word document\n",
    "    document = docx.Document(docx_file)\n",
    "\n",
    "    # Extract table and save to doc file\n",
    "    extract_table_from_docx(docx_file, output_file)\n",
    "\n",
    "    # Close Word document\n",
    "    document.close()\n",
    "\n",
    "\n",
    "# Main program\n",
    "if __name__ == \"__main__\":\n",
    "    # Specify the input and output file paths\n",
    "    input_docx_file = \"test.zip.docx\"\n",
    "    output_doc_file = \"output.docx\"\n",
    "\n",
    "    # Open Word document and extract table\n",
    "    open_word_document_and_extract_table(input_docx_file, output_doc_file)\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same code but now I merge all the document created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from docxcompose.composer import Composer\n",
    "from docx import Document\n",
    "\n",
    "def merge_docx_files(output_file, files_list):\n",
    "    # Create a new Document to serve as the master document\n",
    "    master = Document()\n",
    "\n",
    "    # Create a Composer object with the master document\n",
    "    composer = Composer(master)\n",
    "\n",
    "    # Iterate over the list of files\n",
    "    for file in files_list:\n",
    "        # Open each file as a Document\n",
    "        doc = Document(file)\n",
    "        # Append the content of each file to the master document using the Composer\n",
    "        composer.append(doc)\n",
    "\n",
    "    # Save the merged document\n",
    "    composer.save(output_file)\n",
    "\n",
    "# Usage\n",
    "folder_path = r'.\\sample.docx'\n",
    "files_list = glob.glob(os.path.join(folder_path, '*.docx'))\n",
    "output_file = 'merged.docx'\n",
    "\n",
    "merge_docx_files(output_file, files_list)\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word paragraph extracting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx import Document\n",
    "from docx.enum.text import WD_COLOR\n",
    "import pandas as pd\n",
    "import glob\n",
    "import re\n",
    "\n",
    "# Import keywords from the Excel file and turn them into a list\n",
    "df = pd.read_excel('keywords.xlsx')\n",
    "list_of_words = df['keywords'].tolist()\n",
    "\n",
    "# Set up regex\n",
    "patterns = [r'\\b' + word + r'\\b' for word in list_of_words]\n",
    "patterns.append(r'%')\n",
    "patterns.append(r'€')\n",
    "re_highlight = re.compile('(' + '|'.join(p for p in patterns) + ')+', re.IGNORECASE)\n",
    "\n",
    "def process_docx_files(folder_path):\n",
    "    docx_files = glob.glob(folder_path + '\\\\*.docx')\n",
    "    for file in docx_files:\n",
    "        doc = Document(file)\n",
    "        modified_doc = Document()  # Create a new document to store modified paragraphs\n",
    "        title = doc.core_properties.title  # Extract the title of the document\n",
    "        modified_doc.add_heading(title)  # Add the title as Header 1 to the modified document\n",
    "        for para in doc.paragraphs:\n",
    "            text = para.text\n",
    "            if len(re_highlight.findall(text)) > 0:\n",
    "                matches = re_highlight.finditer(text)\n",
    "                p3 = 0\n",
    "                highlighted_para = modified_doc.add_paragraph()  # Add a new paragraph to the modified document\n",
    "                for match in matches:\n",
    "                    p1 = p3\n",
    "                    p2, p3 = match.span()\n",
    "                    highlighted_para.add_run(text[p1:p2])\n",
    "                    run = highlighted_para.add_run(text[p2:p3])\n",
    "                    run.font.highlight_color = WD_COLOR.YELLOW\n",
    "                    highlighted_para.add_run(text[p3:])\n",
    "        if modified_doc.paragraphs:  # Only save the modified document if it contains highlighted paragraphs\n",
    "            modified_doc.save(file)\n",
    "\n",
    "# Provide the folder path where the algorithm will iterate over all *.docx files\n",
    "folder_path = '.\\\\sample.docx'\n",
    "process_docx_files(folder_path)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test to dowload only table in MS-Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      "Darkness\n",
      "My\n",
      "Friend\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "import docx\n",
    "\n",
    "# Function to retrieve a table from a docx document\n",
    "def retrieve_table_from_docx(file_path):\n",
    "    document = Document(file_path) \n",
    "    tables = document.tables \n",
    "    for table in tables: \n",
    "        for row in table.rows: \n",
    "            for cell in row.cells: \n",
    "                for paragraph in cell.paragraphs: \n",
    "                    print(paragraph.text) \n",
    "    \n",
    "# Use this function in a loop to retrieve tables from multiple docx files\n",
    "\n",
    "# Save the tables to a new docx file with the following function\n",
    "\n",
    "# Create a doc file to store the extracted table\n",
    "def create_doc_file(table, output_file):\n",
    "    document = docx.Document()\n",
    "    new_table = document.add_table(rows=table.rows, cols=table.columns)\n",
    "    for i, row in enumerate(table.rows):\n",
    "        for j, cell in enumerate(row.cells):\n",
    "            new_table.cell(i, j).text = cell.text\n",
    "    document.save(output_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table saved to output.docx\n"
     ]
    }
   ],
   "source": [
    "import docx\n",
    "\n",
    "def retrieve_table_from_docx(file_path):\n",
    "    document = docx.Document(file_path)\n",
    "    tables = document.tables\n",
    "    extracted_table = None\n",
    "    for table in tables:\n",
    "        extracted_table = table  # Store the first table found\n",
    "        break\n",
    "    return extracted_table\n",
    "\n",
    "# Example usage\n",
    "input_file = \"test.docx\"\n",
    "output_file = \"output.docx\"\n",
    "\n",
    "table = retrieve_table_from_docx(input_file)\n",
    "if table is not None:\n",
    "    doc = docx.Document()\n",
    "    new_table = doc.add_table(rows=len(table.rows), cols=len(table.columns))\n",
    "    for i, row in enumerate(table.rows):\n",
    "        for j, cell in enumerate(row.cells):\n",
    "            new_table.cell(i, j).text = cell.text\n",
    "    doc.save(output_file)\n",
    "    print(f\"Table saved to {output_file}\")\n",
    "else:\n",
    "    print(\"No table extracted.\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table saved to output.docx\n"
     ]
    }
   ],
   "source": [
    "import docx\n",
    "\n",
    "def extract_table_from_docx(input_file, output_file):\n",
    "    def retrieve_table_from_docx(file_path):\n",
    "        document = docx.Document(file_path)\n",
    "        tables = document.tables\n",
    "        extracted_table = None\n",
    "        for table in tables:\n",
    "            extracted_table = table  # Store the first table found\n",
    "            break\n",
    "        return extracted_table\n",
    "\n",
    "    table = retrieve_table_from_docx(input_file)\n",
    "    if table is not None:\n",
    "        doc = docx.Document()\n",
    "        new_table = doc.add_table(rows=len(table.rows), cols=len(table.columns))\n",
    "        for i, row in enumerate(table.rows):\n",
    "            for j, cell in enumerate(row.cells):\n",
    "                new_table.cell(i, j).text = cell.text\n",
    "        doc.save(output_file)\n",
    "        print(f\"Table saved to {output_file}\")\n",
    "    else:\n",
    "        print(\"No table extracted.\")\n",
    "\n",
    "# Example usage\n",
    "input_file = \"test.docx\"\n",
    "output_file = \"output.docx\"\n",
    "\n",
    "extract_table_from_docx(input_file, output_file)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using ChatGPT to extract the relevant informations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea of this code is to use chatGPT API to analyse text files with a specific question. Once chatGPT will have done a summary, then even a simple algorythm of keywords should be able to exploit chatGPT analysis and automate a part of the process of data collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les augmentations individuelles de salaires s'élèvent à 3,8% de la masse salariale brute, hors prime d'ancienneté, pour la période du 1er juillet 2022 au 30 juin 2023. Dans le cas d'une augmentation individuelle mensuelle de salaire, le montant minimum est de 60 € brut. En cas de tenue de poste satisfaisante et d'atteinte des objectifs individuels, le montant maximum de l'augmentation individuelle de salaire est de 100 € brut quel que soit le salaire de base du collaborateur. Les augmentations individuelles sont effectives à la date anniversaire de la dernière augmentation, ou, à défaut, de la date d'embauche. Une prime exceptionnelle de 100 € brut est versée à tous les salariés en CDI, en CDD, en contrat d'alternance présents à l'effectif au 30 juin 2022. Les salariés éligibles à une augmentation annuelle individuelle entre juillet 2022 et juin 2023 recevront une prime mensuelle de 50 € brut par mois de juillet 2022 jusqu'au mois précédent la date d'application de l'augmentation individuelle. Une enveloppe budgétaire équivalente à 0,6% de la masse salariale brute hors prime d'ancienneté est consacrée pour les promotions/changements de poste pour la période du 1er juillet 2022 au 30 juin 2023. Les augmentations générales de salaires pour les salariés CSP ne sont pas mentionnées. La répartition des augmentations générales et individuelles suit la logique que les ouvriers-employés en ont plus que les intermédiaires et les cadres, et les intermédiaires plus que les cadres pour les augmentations générales; et pour les augmentations individuelles, les cadres en ont plus que les intermédiaires qui en ont plus que les employés-ouvriers.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "\n",
    "openai.api_key = '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'\n",
    "\n",
    "def BasicGeneration(prompt):\n",
    "    completion = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "        {\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "    return completion.choices[0].message.content\n",
    "\n",
    "with open('text1.txt', 'r') as file:\n",
    "    text = file.read()\n",
    "\n",
    "order = \"Tu es analyste GPT, un lecteur averti. Je vais te donner un texte et tu vas devoir analyser en respectant les consignes suivantes: \\\n",
    "je veux que tu me résumes le texte suivant qui porte sur les accords de négociation annuelle obligatoire de salaire en me donnant les informations suivantes \\\n",
    "si elles sont disponibles (si elles ne le sont pas alors écrit 'information indisponible'). Les augmentations générales de salaires pour les salariés CSP (ouvrier-employé ; intérmédiaires ; cadres). \\\n",
    "Je veux que tu donnes chacune de ces informations en une seule phrase et sur le modèle suivante 'l'augmentation générale accordée aux cadres est de X%'. Le montant d'augmentation que tu dois \\\n",
    "prélever est en pourcent, si un autre résultat t'es donné, ignore le. Attention des termes synonmes à augmentation générale peuvent être employé comme 'masse salariale brute'. \\\n",
    "Aussi prend en compte la donnée suivante les augmentations générales quand elles sont différentes entre les salariés sont réparties de sorte à ce que les ouvriers-employés en est plus que \\\n",
    "les intérmédiaries et cadres, et les intermédiaires plsu que les cadres. Inversement pour les augmentations individuelles (aussi appellées au mérite), quand elles sont différentes entre les \\\n",
    "salariés, alors les cadres en reçoivent plus que les intérmediaires, qui en reçoivent plus que les employés-ouvrier. Voici le texte: \" \n",
    "\n",
    "prompt = order + text\n",
    "\n",
    "response = BasicGeneration(prompt)\n",
    "\n",
    "print(response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tu es analyste GPT, un lecteur averti. Je vais te donner un texte et tu vas devoir analyser en respectant les consignes suivantes: je veux que tu me résumes le texte suivant qui porte sur les accords de négociation annuelle obligatoire de salaire en me donnant les informations suivantes si elles sont disponibles (si elles ne le sont pas alors écrit 'information indisponible'). Les augmentations générales de salaires pour les salariés CSP (ouvrier-employé ; intérmédiaires ; cadres). Je veux que tu donnes chacune de ces informations en une seule phrase et sur le modèle suivante 'l'augmentation générale accordée aux cadres est de X%'. Le montant d'augmentation que tu dois prélever est en pourcent, si un autre résultat t'es donné, ignore le. Attention des termes synonmes à augmentation générale peuvent être employé comme 'masse salariale brute'. Aussi prend en compte la donnée suivante les augmentations générales quand elles sont différentes entre les salariés sont réparties de sorte à ce que les ouvriers-employés en est plus que les intérmédiaries et cadres, et les intermédiaires plsu que les cadres. Inversement pour les augmentations individuelles (aussi appellées au mérite), quand elles sont différentes entre les salariés, alors les cadres en reçoivent plus que les intérmediaires, qui en reçoivent plus que les employés-ouvrier. Voici le texte: T03722003682-44780501100015.docx.docx\n",
      "T03722003682-44780501100015.docx\n",
      "Augmentation individuelle des salaires de base individuelle des salaires de base\n",
      "Au terme des négociations, les parties s'entendent pour l'octroi d'une enveloppe globale consacrée aux augmentations individuelles des salariés de 3,8% de la masse salariale brute, hors prime d'ancienneté, pour la période du 1er juillet 2022 au 30 juin 2023.  globale consacrée aux augmentations individuelles des salariés de 3,8% de la masse salariale brute, hors prime d'ancienneté, pour la période du 1er juillet 2022 au 30 juin 2023.  brute, hors prime d'ancienneté, pour la période du 1er juillet 2022 au 30 juin 2023. \n",
      "Dans le cas où une augmentation individuelle mensuelle de salaire est attribuée, celle-ci sera d'un montant minimum de 60 € brut. Les augmentations individuelles attribuées sont appliquées sur le salaire de base  hors primes, hors ancienneté. Il correspond à la première ligne du bulletin de paie. individuelle mensuelle de salaire est attribuée, celle-ci sera d'un montant minimum de 60 € brut. Les augmentations individuelles attribuées sont appliquées sur le salaire de base  hors primes, hors ancienneté. Il correspond à la première ligne du bulletin de paie. de 60 € brut. Les augmentations individuelles attribuées sont appliquées sur le salaire de base  hors primes, hors ancienneté. Il correspond à la première ligne du bulletin de paie.\n",
      "Par ailleurs, dans le cas d'une tenue de poste satisfaisante et de l'atteinte des objectifs individuels, le montant de l'augmentation individuelle de salaire sera porté à  100 € brut, et ce quel que soit le salaire de base du collaborateur concerné. individuelle de salaire sera porté à  100 € brut, et ce quel que soit le salaire de base du collaborateur concerné.\n",
      "S'il y a lieu, les augmentations individuelles seront effectives à la date anniversaire de la dernière augmentation, ou, à défaut, de la date d'embauche.\n",
      "D'autre part, la Direction s'engage à remettre à chaque collaborateur un courrier pour l'informer, s'il y a lieu, de son augmentation individuelle.\n",
      "Versement d'une prime exceptionnelle liée au niveau d'inflation\n",
      "Afin d'apporter une mesure forte et rapide face au niveau d'inflation constatée, la Direction octroie une prime exceptionnelle à tous les salariés en CDI, en CDD, en contrat d'alternance, présents à l'effectif au 30 juin 2022, pour un montant identique pour tous de 100 € brut. Cette prime sera versée sous forme d'acompte aux salariés dans la première quinzaine du mois de juillet 2022. Une écriture de régularisation sera passée sur le bulletin de paie de juillet 2022.\n",
      "Par ailleurs les parties s'entendent pour le versement différencié d'une seconde partie de cette prime exceptionnelle qui sera fait de la façon suivante : versement, aux collaborateurs qui remplissent les conditions d'éligibilité d'une augmentation annuelle individuelle entre juillet 2022 et juin 2023, de façon lissée dans le temps, d'une prime d'un montant de 50 € brut par mois, à compter de juillet 2022 jusqu'au mois précédent la date d'application de l'augmentation individuelle. Exemple : de juillet 2022 à janvier 2023 pour un collaborateur qui bénéficiera d'une augmentation individuelle en février 2023. qui sera fait de la façon suivante : versement, aux collaborateurs qui remplissent les conditions d'éligibilité d'une augmentation annuelle individuelle entre juillet 2022 et juin 2023, de façon lissée dans le temps, d'une prime d'un montant de 50 € brut par mois, à compter de juillet 2022 jusqu'au mois précédent la date d'application de l'augmentation individuelle. Exemple : de juillet 2022 à janvier 2023 pour un collaborateur qui bénéficiera d'une augmentation individuelle en février 2023. annuelle individuelle entre juillet 2022 et juin 2023, de façon lissée dans le temps, d'une prime d'un montant de 50 € brut par mois, à compter de juillet 2022 jusqu'au mois précédent la date d'application de l'augmentation individuelle. Exemple : de juillet 2022 à janvier 2023 pour un collaborateur qui bénéficiera d'une augmentation individuelle en février 2023. individuelle. Exemple : de juillet 2022 à janvier 2023 pour un collaborateur qui bénéficiera d'une augmentation individuelle en février 2023.\n",
      "Les parties s'entendent pour consacrer une enveloppe budgétaire pour les promotions / changements de poste, équivalente à 0,6% de la masse salariale brute hors prime d'ancienneté, pour la période du 1er juillet 2022 au 30 juin 2023. budgétaire pour les promotions / changements de poste, équivalente à 0,6% de la masse salariale brute hors prime d'ancienneté, pour la période du 1er juillet 2022 au 30 juin 2023. brute hors prime d'ancienneté, pour la période du 1er juillet 2022 au 30 juin 2023.\n",
      "T03722003682-44780501100015.docx.docx\n",
      "T03722003682-44780501100015.docx\n",
      "Augmentation individuelle des salaires de base individuelle des salaires de base\n",
      "Au terme des négociations, les parties s'entendent pour l'octroi d'une enveloppe globale consacrée aux augmentations individuelles des salariés de 3,8% de la masse salariale brute, hors prime d'ancienneté, pour la période du 1er juillet 2022 au 30 juin 2023.  globale consacrée aux augmentations individuelles des salariés de 3,8% de la masse salariale brute, hors prime d'ancienneté, pour la période du 1er juillet 2022 au 30 juin 2023.  brute, hors prime d'ancienneté, pour la période du 1er juillet 2022 au 30 juin 2023. \n",
      "Dans le cas où une augmentation individuelle mensuelle de salaire est attribuée, celle-ci sera d'un montant minimum de 60 € brut. Les augmentations individuelles attribuées sont appliquées sur le salaire de base  hors primes, hors ancienneté. Il correspond à la première ligne du bulletin de paie. individuelle mensuelle de salaire est attribuée, celle-ci sera d'un montant minimum de 60 € brut. Les augmentations individuelles attribuées sont appliquées sur le salaire de base  hors primes, hors ancienneté. Il correspond à la première ligne du bulletin de paie. de 60 € brut. Les augmentations individuelles attribuées sont appliquées sur le salaire de base  hors primes, hors ancienneté. Il correspond à la première ligne du bulletin de paie.\n",
      "Par ailleurs, dans le cas d'une tenue de poste satisfaisante et de l'atteinte des objectifs individuels, le montant de l'augmentation individuelle de salaire sera porté à  100 € brut, et ce quel que soit le salaire de base du collaborateur concerné. individuelle de salaire sera porté à  100 € brut, et ce quel que soit le salaire de base du collaborateur concerné.\n",
      "S'il y a lieu, les augmentations individuelles seront effectives à la date anniversaire de la dernière augmentation, ou, à défaut, de la date d'embauche.\n",
      "D'autre part, la Direction s'engage à remettre à chaque collaborateur un courrier pour l'informer, s'il y a lieu, de son augmentation individuelle.\n",
      "Versement d'une prime exceptionnelle liée au niveau d'inflation\n",
      "Afin d'apporter une mesure forte et rapide face au niveau d'inflation constatée, la Direction octroie une prime exceptionnelle à tous les salariés en CDI, en CDD, en contrat d'alternance, présents à l'effectif au 30 juin 2022, pour un montant identique pour tous de 100 € brut. Cette prime sera versée sous forme d'acompte aux salariés dans la première quinzaine du mois de juillet 2022. Une écriture de régularisation sera passée sur le bulletin de paie de juillet 2022.\n",
      "Par ailleurs les parties s'entendent pour le versement différencié d'une seconde partie de cette prime exceptionnelle qui sera fait de la façon suivante : versement, aux collaborateurs qui remplissent les conditions d'éligibilité d'une augmentation annuelle individuelle entre juillet 2022 et juin 2023, de façon lissée dans le temps, d'une prime d'un montant de 50 € brut par mois, à compter de juillet 2022 jusqu'au mois précédent la date d'application de l'augmentation individuelle. Exemple : de juillet 2022 à janvier 2023 pour un collaborateur qui bénéficiera d'une augmentation individuelle en février 2023. qui sera fait de la façon suivante : versement, aux collaborateurs qui remplissent les conditions d'éligibilité d'une augmentation annuelle individuelle entre juillet 2022 et juin 2023, de façon lissée dans le temps, d'une prime d'un montant de 50 € brut par mois, à compter de juillet 2022 jusqu'au mois précédent la date d'application de l'augmentation individuelle. Exemple : de juillet 2022 à janvier 2023 pour un collaborateur qui bénéficiera d'une augmentation individuelle en février 2023. annuelle individuelle entre juillet 2022 et juin 2023, de façon lissée dans le temps, d'une prime d'un montant de 50 € brut par mois, à compter de juillet 2022 jusqu'au mois précédent la date d'application de l'augmentation individuelle. Exemple : de juillet 2022 à janvier 2023 pour un collaborateur qui bénéficiera d'une augmentation individuelle en février 2023. individuelle. Exemple : de juillet 2022 à janvier 2023 pour un collaborateur qui bénéficiera d'une augmentation individuelle en février 2023.\n",
      "Les parties s'entendent pour consacrer une enveloppe budgétaire pour les promotions / changements de poste, équivalente à 0,6% de la masse salariale brute hors prime d'ancienneté, pour la période du 1er juillet 2022 au 30 juin 2023. budgétaire pour les promotions / changements de poste, équivalente à 0,6% de la masse salariale brute hors prime d'ancienneté, pour la période du 1er juillet 2022 au 30 juin 2023. brute hors prime d'ancienneté, pour la période du 1er juillet 2022 au 30 juin 2023.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('text1.txt', 'r') as file:\n",
    "    text = file.read()\n",
    "\n",
    "order = \"Tu es analyste GPT, un lecteur averti. Je vais te donner un texte et tu vas devoir analyser en respectant les consignes suivantes: \\\n",
    "je veux que tu me résumes le texte suivant qui porte sur les accords de négociation annuelle obligatoire de salaire en me donnant les informations suivantes \\\n",
    "si elles sont disponibles (si elles ne le sont pas alors écrit 'information indisponible'). Les augmentations générales de salaires pour les salariés CSP (ouvrier-employé ; intérmédiaires ; cadres). \\\n",
    "Je veux que tu donnes chacune de ces informations en une seule phrase et sur le modèle suivante 'l'augmentation générale accordée aux cadres est de X%'. Le montant d'augmentation que tu dois \\\n",
    "prélever est en pourcent, si un autre résultat t'es donné, ignore le. Attention des termes synonmes à augmentation générale peuvent être employé comme 'masse salariale brute'. \\\n",
    "Aussi prend en compte la donnée suivante les augmentations générales quand elles sont différentes entre les salariés sont réparties de sorte à ce que les ouvriers-employés en est plus que \\\n",
    "les intérmédiaries et cadres, et les intermédiaires plsu que les cadres. Inversement pour les augmentations individuelles (aussi appellées au mérite), quand elles sont différentes entre les \\\n",
    "salariés, alors les cadres en reçoivent plus que les intérmediaires, qui en reçoivent plus que les employés-ouvrier. Voici le texte: \" \n",
    "\n",
    "prompt = order + text\n",
    "\n",
    "\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **It works**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from docx import Document\n",
    "\n",
    "\n",
    "openai.api_key = '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'\n",
    "\n",
    "\n",
    "def generate_summary(text):\n",
    "    order = \"\"\"Tu es analyste GPT, un lecteur averti. Je vais te donner un texte et tu vas devoir analyser en respectant les consignes suivantes: \\\n",
    "    je veux que tu me résumes le texte suivant qui porte sur les accords de négociation annuelle obligatoire de salaire en me donnant les informations suivantes et uniquement celles-ci \\\n",
    "    si elles sont disponibles: Les augmentations générales de salaires (aussi appellées augmentation collective, ou augmentation de la masse salariale brute) \\\n",
    "    En fonction des catégories socio-professionnelles/statut professionnel des salariés: ouvrier-employé ; Professions intermédiaires (techniciens, agents de maitrise, ou autres) ; cadres/ingénieurs. \\\n",
    "    Je veux que tu donnes une information par phrase sur le modèle suivant 'l'augmentation générale accordée aux cadres est de X%. L'augmentation générale accordée aux ouvriers-employés est de Y%'. \\\n",
    "    Le montant d'augmentation que tu dois prélever est en pourcent ou en euros. \\\n",
    "    Enfin, prend en compte la donnée suivante les augmentations générales quand elles sont différentes entre les salariés sont réparties de sorte à ce que les ouvriers-employés en est plus que \\\n",
    "    les intérmédiaries et cadres, et les intermédiaires plsu que les cadres. Inversement pour les augmentations individuelles (aussi appellées au mérite), quand elles sont différentes entre les \\\n",
    "    salariés, alors les cadres en reçoivent plus que les intérmediaires, qui en reçoivent plus que les employés-ouvrier. Voici le texte: \"\"\"\n",
    "\n",
    "    prompt = order + text\n",
    "    completion = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "    response = completion.choices[0].message.content\n",
    "    return response\n",
    "\n",
    "\n",
    "def save_summary_to_docx(summary, output_file):\n",
    "    file_name = os.path.basename(output_file)\n",
    "    document = Document()\n",
    "    document.add_paragraph(summary)\n",
    "    document.save(file_name)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    with open('text2.txt', 'r') as file:\n",
    "        text = file.read()\n",
    "\n",
    "    summary = generate_summary(text)\n",
    "    save_summary_to_docx(summary, \"output3.docx\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Works fine. However necessary to have very good quality sample and also to check the results. Would work better on a large sample. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "from docx import Document\n",
    "import shutil\n",
    "\n",
    "openai.api_key = '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'\n",
    "\n",
    "def generate_summary(text):\n",
    "    try:\n",
    "        order = \"\"\"Tu es analyste GPT, un lecteur averti. Je vais te donner un texte et tu vas devoir analyser en respectant les consignes suivantes: \\\n",
    "je veux que tu me résumes le texte suivant qui porte sur les accords de négociation annuelle obligatoire de salaire en me donnant les informations suivantes et uniquement celles_ci \\\n",
    "si elles sont disponibles (si elles ne le sont pas alors écrit 'information indisponible'): Les augmentations générales ET individuelles de salaires (aussi appellées augmentation collective/au mérite, ou augmentation de la masse salariale brute)\\ \n",
    "En fonction des catégories socio-professionnelles/statut professionnel des salariés: ouvrier-employé ; Professions intermédiaires (techniciens, agents de maitrise, ou autres) ; cadres/ingénieurs. \\\n",
    "Je veux que tu donnes une information par phrase sur le modèle suivante 'l'augmentation générale accordée aux cadres est de X%. L'augmentation générale accordée aux ouvriers-employés est de Y%'.\\ \n",
    "Le montant d'augmentation que tu dois prélever est en pourcent ou en euros.\\\n",
    "Enfin, prend en compte la donnée suivante les augmentations générales quand elles sont différentes entre les salariés sont réparties de sorte à ce que les ouvriers-employés en est plus que \\\n",
    "les intérmédiaries et cadres, et les intermédiaires plsu que les cadres. Inversement pour les augmentations individuelles (aussi appellées au mérite), quand elles sont différentes entre les \\\n",
    "salariés, alors les cadres en reçoivent plus que les intérmediaires, qui en reçoivent plus que les employés-ouvrier. Voici le texte: \"\"\"\n",
    "\n",
    "        prompt = order + text\n",
    "        completion = openai.ChatCompletion.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        )\n",
    "        response = completion.choices[0].message.content\n",
    "        return response\n",
    "    except:\n",
    "        print(\"Error\")\n",
    "\n",
    "def save_summary_to_docx(summary, output_file):\n",
    "    file_name = os.path.basename(output_file)\n",
    "    document = Document()\n",
    "    document.add_paragraph(summary)\n",
    "    document.save(file_name)\n",
    "\n",
    "def move_files(source_folder, destination_folder):\n",
    "    for filename in os.listdir(source_folder):\n",
    "        if filename.endswith(\".docx\") and (filename.startswith(\"T\") or filename.startswith(\"A\")):\n",
    "            file_path = os.path.join(source_folder, filename)\n",
    "            shutil.move(file_path, destination_folder)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    folder_path = r\".\\sample.txt\"\n",
    "    output_folder = r\".\\summaryGPT\"\n",
    "\n",
    "    # Iterate over the files in the folder\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            \n",
    "            # Read the text from the file\n",
    "            with open(file_path, 'r') as file:\n",
    "                text = file.read()\n",
    "\n",
    "            # Generate the summary\n",
    "            summary = generate_summary(text)\n",
    "\n",
    "            # Save the summary to a DOCX file\n",
    "            output_file = os.path.join(output_folder, filename.replace(\".txt\", \".docx\"))\n",
    "            save_summary_to_docx(summary, output_file)\n",
    "\n",
    "            # Move to the right folder\n",
    "            move_files(r\"C:\\Users\\garsonj\\Desktop\\NAO_sample\", r\".\\summaryGPT\")\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Move files to the right folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "def move_files(source_folder, destination_folder):\n",
    "    for filename in os.listdir(source_folder):\n",
    "        if filename.endswith(\".docx\"):\n",
    "            file_path = os.path.join(source_folder, filename)\n",
    "            shutil.move(file_path, destination_folder)\n",
    "\n",
    "move_files(r\"C:\\Users\\garsonj\\Desktop\\NAO_sample\", r\".\\summaryGPT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from docx import Document\n",
    "import shutil\n",
    "\n",
    "openai.api_key = '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'\n",
    "\n",
    "\n",
    "def generate_summary(text):\n",
    "    try:\n",
    "        order = \"\"\"Tu es analyste GPT, un analyste expérimenté. Cela fait 10 ans que tu lis chaque accord de négociation annuelle obligatoire pour détecter les dynamiques salariales.\\\n",
    " Je vais te donner un texte et tu vas devoir résumer les points suivants et uniquement ceux-ci \\\n",
    "si ils sont disponibles (si non écrit 'information indisponible'): \\\n",
    "- Les augmentations générales. Attention beaucoup de synonyme comme \"augmentation brute\" ou \"du salaire de base\" \\En fonction des catégories socio-professionnelles/statut professionnel des salariés: ouvrier-employé ; Professions intermédiaires (techniciens, agents de maitrise, ou autres) ; cadres/ingénieurs.Le montant d'augmentation que tu dois prélever est en pourcent ou en euros. Il peut parfois être indiqué par rapport au SMIC ou à des grades/échelons allant de 1 à 9\\\n",
    "- Les augmentations individuelles de salaires.  Attention beaucoup de synonyme comme \"au mérite\", \\En fonction des catégories socio-professionnelles/statut professionnel des salariés: ouvrier-employé ; Professions intermédiaires (techniciens, agents de maitrise, ou autres) ; cadres/ingénieurs.Le montant d'augmentation que tu dois prélever est en pourcent ou en euros. Il peut parfois être indiqué par rapport au SMIC ou à des grades/échelons allant de 1 à 9\\\n",
    "- les primes de partage de la valeur ajoutée, ou PEPA, ou PPV\n",
    " \\\n",
    "Donne une information par phrase et une phrase par bullet point\\\n",
    "Enfin, prend en compte la donnée suivante les augmentations générales quand elles sont différentes entre les salariés sont réparties de sorte à ce que les ouvriers-employés en est plus que \\\n",
    "les intérmédiaries et cadres, et les intermédiaires plus que les cadres. Inversement pour les augmentations individuelles (aussi appelles au mérite), quand elles sont différentes entre les \\\n",
    "salariés, alors les cadres en reçoivent plus que les intérmediaires, qui en reçoivent plus que les employés-ouvrier. \\\n",
    "suis le modèle suivant # Augmentation générale # augmentation individuelle # Primes\\\n",
    "\n",
    "Voici le texte: \"\"\"\n",
    "\n",
    "        prompt = order + text\n",
    "        completion = openai.ChatCompletion.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        )\n",
    "        response = completion.choices[0].message.content\n",
    "        return response\n",
    "    except:\n",
    "        print(\"Error\")\n",
    "\n",
    "\n",
    "def save_summary_to_docx(summary, output_file):\n",
    "    file_name = os.path.basename(output_file)\n",
    "    document = Document()\n",
    "    document.add_paragraph(summary)\n",
    "    document.save(file_name)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    folder_path = r\".\\sample.txt\"\n",
    "    output_folder = r\".\\summaryGPT\"\n",
    "\n",
    "    # Create the \"summaryGPT\" folder if it doesn't exist\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Iterate over the files in the folder\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "            # Read the text from the file\n",
    "            with open(file_path, 'r') as file:\n",
    "                text = file.read()\n",
    "\n",
    "            # Generate the summary\n",
    "            summary = generate_summary(text)\n",
    "\n",
    "            # Save the summary to a DOCX file\n",
    "            output_file = os.path.join(output_folder, filename.replace(\".txt\", \".docx\"))\n",
    "            save_summary_to_docx(summary, output_file)\n",
    "\n",
    "            # Move the file to the \"summaryGPT\" folder and overwrite if duplicate exists\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cleaning function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import os\n",
    "\n",
    "def clean_text(file_path):\n",
    "    stop_words = set(stopwords.words('french'))\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        text = file.read()\n",
    "\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    cleaned_text = ' '.join(filtered_words)\n",
    "\n",
    "    with open(file_path, 'w') as file:\n",
    "        file.write(cleaned_text)\n",
    "\n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "\n",
    "directory = r\".\\sample.txt\"\n",
    "\n",
    "for file in os.listdir(directory):\n",
    "    if file.endswith(\".txt\"):\n",
    "        file_path = os.path.join(directory, file)\n",
    "        clean_text(file_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
